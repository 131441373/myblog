# Loss函数

预测时用beam search

## CTC 
对于语音问题, 切成若干帧段
对于图象, 横向切成若干细高小矩形?
反正都可以理解为 $T$ 个时间片

**三个主要假设**
- 不同时间片的输出是time independent的 (这个必然存在一点问题, 但是解决起来也比较困难的)
- 时间片与真实输出是单调对应的 (这个对于其目标任务语音识别和ocr都是成立的, 不会出现倒序问题, 不像translation这种
- label的长度小于时间片数量 (这个在语音识别和ocr人物中应该都没有问题的)

可以想象机器识别出 CCAAATT -> CAT
一个简单的想法是把识别出的结果中的duplicated的字符去重
但是这样是无法产生重复字符的结果的
所以引入一个 $\epsilon$ 空白标签, 去重时不跨过 $\epsilon$ 即可
这种去重法称作 collpase, 记为 $\mathcal{B}$

我们的network生成的是collapse前的, 而非collapse后的

collapse前我们有一个under model parameter $\omega$ 的 distribution, 为: (其中 y为在时刻t预测词$\pi_t$的概率, **假设了这个输出时序无关**

$$p(\pi|x; \omega) = \prod_{t=1}^T P(\pi_t | t, x; \omega)$$

不难发现如果该分布中大量的结果collapse后都指向某个指定的结果res
那么那个res就是我们要的东西

根据上面我们定义 $p(l|x)$ 去统计collapse前的distribution中指向某个特定结果 $l$ 的可能性有多大

$$p(l|x; \omega) := \sum_{\pi \in \mathcal{B}^{-1}(l)} p(\pi|x; \omega)$$

记 $L^{\le T}$ 为所有长度 $\le T$ 的序列, 用于表示collaspe后的结果

令 $l$ 为ground true, 然后令上式最小化
dp
将 $\pi$ 视为一条路径, 不难联想到HMM中的dp算法

注意到collapse前的序列, 增加一个字符, 对应的collapse结果不可能长度减少, 所以符合dp性质
记 $\alpha_t(s)$ 为用了t个时间片, collapse后能匹配上 $l[:s]$

## Attention

## ACE (Aggregation Cross-Entropy)
1D预测: 生成一个文字序列
2D预测: 图上对应位置标字 (将 H*W 压扁成 T=HW 变为1D)

- CTC很难做出2D预测, ACE可
- 无需dp, 几乎无需额外空间
- 效率高
- 不考虑序列, 只考虑个数

记 $N_k$ 为 gt 的标注中, 类 $k$ 的个数, 特别的 $\epsilon$ 对应的类 个数为 $T-非空标注数$
记 $y_k = \sum_{t=1}^T y_k^t$ (即使用期望个数来代替真实个数), $\epsilon$ 同理处理

然后二者都除以 $T$ 做一个标准化
类似带概率的二分类cross entropy
这里loss公式为

$$L = \sum_{k=1}^{|C^{\epsilon}|} \overline {N_k} \times -\log {\overline{y_k}}$$
