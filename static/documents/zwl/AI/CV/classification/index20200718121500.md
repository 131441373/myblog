
### ideas
1）图片去掉奇数行和奇数列，变为1/4大小的图，看上去没有太大区别 (pooling)
2）用于识别的特征（如鸟嘴，鸡爪）一般相比整张图片小得多 (filter)
3）2中的特征因而可能出现在左上、左下、中、右等等一张图片的各种不同的方位 (滑动窗口)

### 基本结构
conv - pooling - conv - pooling -... repeat

然后 - flatten - classification

#### 基本术语
filter（又称kernal（卷积核））为滑动窗口，窗口内的值与filter中的值内积（rgb图是立方体，filter也对应是立方体）
stride为滑动的步频（左右的、上下的），比如横向滑动时，一次滑动几格，纵向滑动时，一次滑动几格

#### pooling (down sampling)
使得表示更小, 但信息尽量不丢失

常规pooling的stride就是边长，即各块pool之间不相交, 以达到取样效果
近来也有进行尝试stride不等于pooling边长的

一个处理方式是每块取max,  称为max pooling  [maxpool 理解 link](https://blog.csdn.net/kele_imon/article/details/79532286)
max pooling在BP时, 只将grad传播给max对应的那个id处, 其他位置不传

另一种处理方式是每块取mean, 称为 mean pooling

不同depth之间的处理无关, 所以depth不变
对于N*M*D的立方体,  n*m大小的filter, 设置pad的padding, stride步长
pooling后获得 [(N + 2*pad_n - n) / stride_n + 1] * [(M + 2*pad_m - m) / stride_m + 1] * D的立方体

#### conv

对于一个filter
其滑过的每个区域, 在下一层对应建一个神经元, 那个神经元与区域中的所有层的元素连边
由于是同一个filter,  所以上述神经元中, 在filter中相对位置相同的边, 共享一个parameter
如下图: 同一个颜色的边表示共享的一个参数
(有时可以连多一个bias点, 同理共享参数)

对于多个filter, 神经元在厚度上进行堆叠
对于N*M*D的立方体,  k个n*m大小的filter, 设置pad的padding, stride步长
conv后获得 [(N + 2*pad_n - n) / stride_n + 1] * [(M + 2*pad_m - m) / stride_m + 1] * k的立方体

![conv层结构](https://img2020.cnblogs.com/blog/1086046/202005/1086046-20200504163430921-138273980.png)

##### bottleneck
filter 1*1大小, stride=1,  filter个数小于原D
得到的结果长宽不变,厚度减少
可以用于减少计算量(幅度还挺大的)

##### mlpconv
进行conv后, 再进行若干次1*1大小filter的conv (中间穿插激活函数
1*1conv可以理解为将输入各层信息进行一个组合
```python
def nin_block(in_channels, out_channels, kernel_size, stride, padding):
    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),
                        nn.ReLU(),
                        nn.Conv2d(out_channels, out_channels, kernel_size=1),
                        nn.ReLU(),
                        nn.Conv2d(out_channels, out_channels, kernel_size=1),
                        nn.ReLU())
    return blk
```

#### 边缘处理
filter和maxpool对于边缘的处理与toolkit相关，pytorch, tensorflow, caffe等处理方式不一
pytorch的处理方式为 [link](https://blog.csdn.net/qxqsunshine/article/details/86435404)

###现有模型

#### LeNet / AlexNet
就是conv, pool, ReLU的简单叠加, 层数比较浅
处理完后, 全连接跑classification

##### 全局平均池化GAP代替全连接层
global average pooling: 其实就是filter的大小就是input的 长*宽 (全局的意思)
模型以卷积层结束，该卷积层生成**与目标类数量一样多(指depth)**的特征图
并对每个特征集应用全局平均池化，以便将每个特征图转换为一个值
这样就可以啦(需要时补个softmax就行

#### VCGNet中的非线性conv思想
对于K个7*7, stride=1的filter的conv
VCGnet将其表示为3层, 每层K个 3*3且 stride=1 的filter
由于stride=1
第三层的3*3对应了第二层的一个5*5,进而对应第一层的一个7*7, 同时还增加了一定的非线性性 
不仅如此, 正比于7*7的参数 和 正比于3*3*3的参数进行比较,  后者更小

**也即, 用更少的参数, 表现出了更多的性质**

#### GoogleNet中层数多但反而高效思想
[paper](https://arxiv.org/pdf/1409.4842.pdf)

上一层到下一层
平行的进行不同大小filter的conv 或 不同大小的pool, 甚至几个conv几个pool一起 (通过调整padding使得不同size的filter得到相同height*width结果)
得到的结果在厚度上进行堆叠到下一层

需要用到bottleneck压计算量, 不然计算量很大

可以这么理解
有的特征可以先在3*3发现某个轮廓, 然后在5*5发现一个特征
有的特征可以先在5*5发现某个轮廓, 然后在3*3发现一个特征

googleNet的结构 发现特征的方式数量 随着层数的增多以指数级别提升

#### ResNet(152层超级无敌深DNN)
一个基础的想法是将VCGNet中的堆叠进一步堆叠
然而, 由于数据量的明显不足, 超深的DNN面临的不是overfit(high variance)的问题, 而是high bias(underfit)的问题

由于多层的DNN将某层开始的所有层设成identity层, 那么深层DNN至少是可以达到浅层DNN的效果的
也就是说, 可以在深层DNN的学习方法上做出改进

ResNet对这种超深网络进行了特别的处理
假设原来要训练的是H(x)
现在既然identity层可以, 就先给他连一个identity,  然后训练**残差** F(x)=H(x)-x, 于是 H(x) = F(x)+x, 表示在NN上就是在原网络上加一条identity边

有几种解释方法:
1) 解决了梯度破碎的问题: 有学者指出, 随着深度的增加, 神经元之间的梯度相关性指数级减少, 梯度空间的结构也随着深度增加而消除
2) 集成学习: 残差方式提供了不同的路径, 带来了一定的独立性和冗余性
[这有一篇文章整理了一些不同的理解方式及对应论文](https://zhuanlan.zhihu.com/p/80226180)

##### 基于ResNet的改进
1) identity边的位置设置
2) 使用L1/L2正则化, 使得ResNet中不必要的参数趋于0, (某些不必要的地方, 就保持和identity接近即可)
3) wide代替depth, 一层中的filter数量提升, 压小层数
4) 使用googleNet思路一层多路径
5) 随机dropout一整个resnet块(被identity跨过的区域称为块)
6) DenseNet块代替resnet块, 这一块中,  每层identity边向之后的每一层都连
详见https://www.bilibili.com/video/BV1Dx411n7UE?p=9 第9集后面

#### 已有算法对比
纵向为准确度, 横向为时间复杂度, 圆的面积是空间复杂度
![图](https://img2020.cnblogs.com/blog/1086046/202007/1086046-20200710152433566-1597383395.png)


## 模板
digit-recognizer
MNIST数据集
```python
import numpy as np
import pandas as pd
import torch.nn as nn
import torch.utils.data as Data
import os
import torch

def transform(datas): # 784 -> 28 * 28
    imgs = []
    for _, data in datas.iterrows():
        imgs.append([data.to_numpy().reshape(28, 28)]) # [ ] 1个channel也要显式的表现出来

    return imgs

def input_train(path):
    def translate(datas):
        Y = torch.tensor(datas['label'].values, dtype=torch.int64)
        X = torch.tensor(transform(datas.drop(columns='label')), dtype=torch.float32) / 255.0
        return X.cuda(), Y.cuda()

    datas = pd.read_csv(path)
    t = datas.shape[0] // 10
    l = t * 6
    r = t * 8
    return translate(datas.iloc[:l]), translate(datas.iloc[l:r]), translate(datas.iloc[r:])

def input_submission(path):
    def translate(datas):
        X = torch.tensor(transform(datas), dtype=torch.float32) / 255.0
        return X.cuda()

    datas = pd.read_csv(path)
    return translate(datas)

class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.CNN1 = nn.Sequential(
            nn.Conv2d(1, 20, 5),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.CNN2 = nn.Sequential(
            nn.Conv2d(20, 50, 5),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.Flat = nn.Flatten()
        self.Classification = nn.Sequential(
            nn.Linear(800, 500),
            nn.ReLU(),
            nn.Linear(500, 10),
        )
    def forward(self, x):
        x = self.CNN1(x)
        x = self.CNN2(x)
        x = self.Flat(x)
        x = self.Classification(x)
        return x

if __name__ == '__main__':
    
    # basic
    (trainX, trainY), (valX, valY), (testX, testY) = input_train("../input/digit-recognizer/train.csv")
    submitX = input_submission("../input/digit-recognizer/test.csv")
    
    fout = open('/kaggle/working/submission.csv', 'w')
    params_path = '/kaggle/working/params.ckpt'
    ckpt_reuse = True # 模型沿用
    model = Net().cuda() # gpu
    if ckpt_reuse and os.path.exists(params_path):
        model.load_state_dict(torch.load(params_path))

    # train
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters())
    loader = Data.DataLoader(
        dataset = Data.TensorDataset(trainX, trainY),
        batch_size = 10,
        shuffle = True
    )
    for epoch in range(20):
        for x, y in loader:
            prediction = model(x)
            loss = criterion(prediction, y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        prediction = model(trainX)
        loss = criterion(prediction, trainY)
        print("epoch{} : {}".format(epoch, loss))

    torch.save(model.state_dict(), params_path)

    # validation
    prediction = torch.argmax(model(valX), 1)
    accuracy = (prediction == valY).sum().float() / valY.shape[0]
    print("validation : {}".format(accuracy))
    
    # submission (kaggle提交)
    print(submitX.shape[0])
    prediction = torch.argmax(model(submitX), 1)
    print(prediction.shape[0])
    fout.write("ImageId,Label\n")
    for i, v in enumerate(prediction):
        fout.write('{},{}\n'.format(i+1, v))
        # if i % 1000 == 0: print(i)
    fout.close() # important!!!!
```