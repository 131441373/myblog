针对 多个object的location

如: 给定一个你关注的object集合, 然后将图上所有你关注的object框起来 (如天空/草这些背景就不关注了

下文中记 bbox = bounding box (最小外接矩形)




### R-CNN (region)
使用SS(selective search)选定约2k的矩形区域RoI(region of interest)
然后每个区域跑SVM 和 bbox regression

问题一: SS正确率感人
问题二: 2k个RoI很可能有重复性, 独立的提取特征有太多的运算重复

### Fast R-CNN
针对问题二: 将特征提取直接在原图上进行, 这样CNN只用跑一次 ( R-CNN没有这么做也可能是当时没有RoI Pooling, 图片的resize在bp时不能求导

仍然在原图上SS, 然后将RoI区域通过CNN各层的shape**映射**到CNN输出层上的一个区域

即: 将CNN与SS解耦了

### Anchor设计
固定面积, 设置ratio = 1, 0.5, 2
使得在固定面积的情况下, 长宽比为ratio
(固定面积相对固定短边的优势: 框更丰富)

#### Anchor用于训练

$n_a$个anchor 和 $n_b$ 个 gt(ground true), $n_a\ge n_b$
按照如下规则匹配

1.
定义 $X_{i,j} = IoU(anchor_i, gt_j)$
![](https://d2l.ai/_images/iou.svg)
每次取出X中的最大元素, 将该格对应anchor和gt匹配, 然后删除X的该行和该列
这样一直操作下去, 最后每个gt都会匹配一个anchor, 然后剩下一些anchor

2.
这些剩下的anchor, 找其IoU最大的gt, 如果IoU值超过某个threshold, 则将其与gt匹配, 否则将其标记为background

对于匹配了gt的anchor
定义其offset(偏差)为

$$\left( \frac{ \frac{x_b - x_a}{w_a} - \mu_x }{\sigma_x}, \frac{ \frac{y_b - y_a}{h_a} - \mu_y }{\sigma_y}, \frac{ \log \frac{w_b}{w_a} - \mu_w }{\sigma_w}, \frac{ \log \frac{h_b}{h_a} - \mu_h }{\sigma_h}\right)$$

3.
负样本(分配为背景的样本可能过多, 导致NN过于重视区分负样的的分类
需要用 难例发掘 优化一下:
即将负样本按照loss从高到低排序(loss高即大概率认为是object), 从高loss的开始取使得 正负样本比约 1:3
(难分辨得解决了简单分辨的一般就不成问题)

默认值 $\mu_x = \mu_y = \mu_w = \mu_h = 0, \sigma_x=\sigma_y=0.1, and, \sigma_w=\sigma_h=0.2$

#### Anchor用于预测
不同的anchor加上offset后得到的predicted bbox可能非常相近, 这在预测的时候显得多余
于是要用的NMS技术(non-maximum suppression)(非极大值抑制: 抑制非极大值)

方法是每次取出置信度(概率)最高的predicted bbox, 然后将与之IoU大于某个阈值(0.3~0.5的超参数)的predicted bbox删掉

不难发现这种做法有个漏洞: 重合的框可能是两个object
举个例子: 一群人拍毕业照, 很可能一个人的半个身子被另一个人挡住, 然后两个框就显得非常相近

soft-nms进行了一些调整:
nms的删除可以理解为将p设为0, 即
$p_i =\left ( \begin{matrix} p_i & IoU(M, box_i) < t \\0& else \end{matrix} \right. $
而在soft-nms中, 而是调整分数, 但不删除
有两种方式:
线性的:
$p_i =\left ( \begin{matrix} p_i & IoU(M, box_i) < t \\ p_i (1 - IoU)& else \end{matrix} \right. $
非线性的:
$p_i =p_i e^{\frac {-IoU^2}{\sigma}} $

### Faster R-CNN
针对问题一: 将SS换成RPN(region proposal network
其提出的 RoI 量少质优

首先仍如Fast R-CNN, 先跑一个CNN得到feature map
然后对feature map上的任意一个pixel加9个anchor
![如图](https://picb.zhimg.com/80/v2-7abead97efcc46a3ee5b030a2151643f_1440w.jpg)
首先我们要训练RPN, 这样构造其训练集
如果anchor还原到原图后, 与某个


最后, 留下的所有anchor中, p>某个阈值的才作为结果

### SSD
single shot detection
![如图](https://pic2.zhimg.com/v2-07eda75a3c5119defb2a13f7f6fe6817_b.jpg)
![如图](https://pic3.zhimg.com/80/v2-6e73f4f987013d933744bf70045b3aa8_1440w.jpg)

https://zhuanlan.zhihu.com/p/79854543

核心思想就是: 多尺度检测, 将CNN划分成若干部分, 每个部分的output都跑一下检测
越靠近图片的层, 用于检测越小的东西

### YOLO v3
主干线也是SSD那样 多尺度
下图中, 图片的尺寸256*256不是关键信息, 可忽略或自行调整大小
![如图](https://pic1.zhimg.com/80/v2-d2596ea39974bcde176d1cf4dc99705e_1440w.jpg)


### R-FCN

### SSD模板
```
# %% [markdown]
# Import:

# %% [code]
from pathlib import Path
import torch
import torch.nn as nn
import torch.utils.data as Data
import torchvision
import numpy as np
import random
import matplotlib.pyplot as plt
import colorsys
import albumentations as aug
import cv2
import torch.nn.functional as F
from PIL import Image as PIL

def apply_cuda(*args):
    if torch.cuda.is_available():
        args = tuple(map(lambda x: x.cuda(), args))
    return args if len(args) > 1 else args[0]

# %% [markdown]
# Config:

# %% [code]
classes = ["background", "aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat", "chair", "cow", "diningtable", "dog", "horse", "motorbike", "person", "pottedplant", "sheep", "sofa", "train", "tvmonitor"]
num_cls = len(classes)
input_edge = 300
input_shape = (input_edge, input_edge)
root = Path().resolve().parent # 本地的话用 Path(__file__)
data_kind = 'pascal_voc'
sizes = [[0.1, 0.2], [0.2, 0.37], [0.37, 0.54], [0.54, 0.71], [0.71, 0.88], [0.88, 0.961]]
ratios = [[1, 2, 0.5]] + [[1, 2, 0.5, 3, 1/3]] * 3 + [[1, 2, 0.5]] * 2
num_anchors = [len(sizes[i]) + len(ratios[i]) - 1 for i in range(6)]

# %% [markdown]
# Image Processing:

# %% [code]
BOX_COLORs = [(x / num_cls, 1., 1.) for x in range(num_cls)]  # hsv form
BOX_COLORs = list(map(lambda x: colorsys.hsv_to_rgb(*x), BOX_COLORs))
BOX_COLORs = list(map(lambda rgb01: tuple(map(lambda x: int(x * 255), rgb01)), BOX_COLORs))
PIL2Tensor = torchvision.transforms.ToTensor()
TEXT_COLOR = (255, 255, 255)
augment = aug.Compose(
    [
        aug.HorizontalFlip(),
        aug.ShiftScaleRotate(p=0.3),
        aug.RandomResizedCrop(input_shape[0], input_shape[1], p=1),
    ],
    bbox_params=aug.BboxParams(format=data_kind, label_fields=['cls_ids'], min_visibility=0.3),
)

def add_bbox(img, bbox, name, color):
    box = tuple(map(int, bbox))
    cv2.rectangle(img, box[:2], box[2:],
                  color=color, thickness=2
    )
    # 图象框, 无填充

    (text_width, text_height), _ = cv2.getTextSize(name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)
    cv2.rectangle(img, (box[0], box[1] - int(1.3 * text_height)), (box[0] + text_width, box[1]),
                  color, thickness=-1
    )
    # 文字背景, 填充

    cv2.putText(img,
                text=name,
                org=(box[0], box[1] - int(0.3 * text_height)),
                fontFace=cv2.FONT_HERSHEY_SIMPLEX,
                fontScale=0.35,
                color=TEXT_COLOR,
                lineType=cv2.LINE_AA,
    )
    # 填入文字
    return img

class Image():
    def __init__(self, arg):
        if isinstance(arg, str):
            self.img = cv2.cvtColor(cv2.imread(arg), cv2.COLOR_BGR2RGB)
        else:
            self.img = arg

    def ToTensor(self):
        return PIL2Tensor(PIL.fromarray(self.img))

    # 绝对位置 和 相对位置 转换
    def abs2rel(self, bboxes):
        h, w = self.img.shape[:2]
        return list(map(lambda b: [b[0]/w, b[1]/h, b[2]/w, b[3]/h], bboxes))

    def rel2abs(self, bboxes):
        h, w = self.img.shape[:2]
        return list(map(lambda b: [b[0]*w, b[1]*h, b[2]*w, b[3]*h], bboxes))

    def augmentation(self, bboxes, clss):
        res = augment(image=self.img, bboxes=bboxes, cls_ids=clss)
        self.img = res["image"]
        return self.abs2rel(res["bboxes"]), res["cls_ids"]

    def visualize(self, bboxes, cls_ids):
        img = self.img.copy()
        for i, bbox in enumerate(bboxes):
            id = cls_ids[i]
            name = classes[id]
            img = add_bbox(img, bbox, name, BOX_COLORs[id])
        plt.figure(figsize=(12, 12))
        plt.imshow(img)

# %% [markdown]
# Multibox Utils:

# %% [code]
def encode(pr, gt): # calcOffset(prior, ground_truth)
    wh_pr = pr[:, 2:] - pr[:, :2]
    wh_gt = gt[:, 2:] - gt[:, :2]
    xy_offset = (gt[:, :2] - pr[:, :2]) / 0.1 / wh_pr
    wh_offset = torch.log(wh_gt / wh_pr) / 0.2
    return torch.cat((xy_offset, wh_offset), dim=1)

def decode(pr, off):  # inverseOffset(prior, offset) -> pred
    wh_pr = pr[:, 2:] - pr[:, :2]
    xy_pred = off[:, :2] * wh_pr * 0.1 + pr[:, :2]
    wh_pred = torch.exp(off[:, 2:] * 0.2) * wh_pr
    return torch.cat((xy_pred, xy_pred + wh_pred), dim=1)

def area(A): # 求每个矩形的面积
    return (A[:, 2] - A[:, 0]) * (A[:, 3] - A[:, 1])

def intersect(A, B): # 求任二矩形的交面积
    n, m = A.shape[0], B.shape[0]
    X = A.unsqueeze(1).expand(n, m, 4) #按列复制成矩形
    Y = B.unsqueeze(0).expand(n, m, 4) #按行复制成矩形
    lu_xy = torch.max(X[:, :, :2], Y[:, :, :2])
    rb_xy = torch.min(X[:, :, 2:], Y[:, :, 2:])
    inter = torch.clamp(rb_xy - lu_xy, min=0)
    return inter[:, :, 0] * inter[:, :, 1]

def Jaccard(A, B): # 任二矩形求IoU
    inter = intersect(A, B)
    X = area(A).unsqueeze(1).expand_as(inter)
    Y = area(B).unsqueeze(0).expand_as(inter)
    union = X + Y - inter
    return inter / union

def getAnchors(w, h, size, ratio):
    sw, sh = 1. / w, 1. / h
    bx, by = 1. / (2 * w), 1. / (2 * h)
    dw, dh = [], []
    ratio = np.array(ratio) ** 0.5
    for i in range(len(ratio)):
        dh.append(size[0] / ratio[i])
        dw.append(size[0] * ratio[i])
    for i in range(1, len(size)):
        dh.append(size[i] / ratio[0])
        dw.append(size[i] * ratio[0])
    res = []
    for j in range(h): # 先h再w 是为了适配 tensor
        for i in range(w):
            ctr = (bx + i * sw, by + j * sh)
            for k in range(len(dh)):
                res.append([ctr[0] - dw[k], ctr[1] - dh[k], ctr[0] + dw[k], ctr[1] + dh[k]])
    return apply_cuda(torch.Tensor(res))

def match_(anchors, gt_bboxs, gt_clss):
    if gt_bboxs.shape[0] == 0:
        return apply_cuda(torch.zeros((anchors.shape[0]), dtype=torch.int64), torch.zeros((anchors.shape[0], 4)))
    X = Jaccard(gt_bboxs, anchors)
    best_pr, best_pr_idx = X.max(1) # 按行
    best_gt, best_gt_idx = X.max(0) # 按列
    best_gt.index_fill(0, best_pr_idx, 2) # 2>threshold即可
    for j in range(best_pr_idx.shape[0]):
        best_gt_idx[best_pr_idx[j]] = j # todo ???????????????????????

    cls_labels = gt_clss[best_gt_idx]
    cls_labels[best_gt < 0.5] = 0

    bbox_offsets = encode(anchors, gt_bboxs[best_gt_idx])

    return cls_labels, bbox_offsets

def match(anchors, gt_bboxs, gt_clss):
    batch_size = len(gt_bboxs)
    cls_labels, bbox_offsets = [None]*batch_size, [None]*batch_size
    for i in range(batch_size):  # numbef of batch
        cls_labels[i], bbox_offsets[i] = match_(anchors, gt_bboxs[i], gt_clss[i])

    cls_labels = torch.stack(cls_labels, dim=0)
    bbox_offsets = torch.stack(bbox_offsets, dim=0)

    return apply_cuda(cls_labels, bbox_offsets)

def IoU(A, B):
    max(A[0], B[0], A)

def NMS(boxs, scores, threshold):
    bboxs, clss_prob = [], []
    while True:
        idx = torch.argmax(scores)
        if scores[idx] < threshold: break
        bboxs.append(boxs[idx])
        clss_prob.append(scores[idx].clone())
        t = Jaccard(boxs, bboxs[-1].unsqueeze(0))
        for i in range(boxs.shape[0]):
            if t[i][0] > 0.4:
                scores[i] *= (1 - t[i][0])
    if len(bboxs) == 0:
        return apply_cuda(torch.Tensor([]), torch.Tensor([]))
    return apply_cuda(torch.stack(bboxs, dim=0), torch.Tensor(clss_prob))

def Detect(cls_preds, box_preds, threshold = 0.7):
    bboxes, clss = [None]*(num_cls-1), [None]*(num_cls-1)
    for c in range(1, len(classes)):
        mask = cls_preds[c].gt(0.01)
        scores = cls_preds[c][mask]
        if scores.shape[0] == 0: 
            bboxes[c-1], clss[c-1] = apply_cuda(torch.Tensor([]), torch.LongTensor([]))
            continue
        boxs = box_preds[mask]
        boxs, prob = NMS(boxs, scores, threshold)
        bboxes[c-1] = boxs[prob > threshold]
        clss[c-1] = apply_cuda(torch.LongTensor([c]*len(boxs)))
    return torch.cat(bboxes, dim=0), torch.cat(clss, dim=0)

# %% [markdown]
# Dataset:

# %% [code]
class VOC_Train_Dataset(Data.Dataset): # todo cls_idx+1
    def __init__(self, kind):
        path = root/"input"/"get-data-voc-2012-dataset"/f"{kind}.txt"
        f = path.open('r')
        img_list = f.read().strip().split('\n')
        f.close()
        self.total = len(img_list)
        self.img_pth, self.gt_bbox, self.gt_cls = [], [], []
        for l in img_list:
            l = l.split(' ')
            self.img_pth.append(l[0])
            self.gt_bbox.append([])
            self.gt_cls.append([])
            for info in l[1:]:
                info = tuple(map(int, info.split(',')))
                self.gt_bbox[-1].append(info[:4])
                self.gt_cls[-1].append(info[4])

    def __len__(self):
        return self.total

    def __getitem__(self, idx):
        img = Image(self.img_pth[idx])
        #img.visualize(self.gt_bbox[idx], self.gt_cls[idx])
        bboxes, clss = img.augmentation(self.gt_bbox[idx], self.gt_cls[idx])
        #img.visualize(bboxes, clss)
        return apply_cuda(img.ToTensor(), torch.Tensor(bboxes), torch.LongTensor(clss))

def collate_fn(batch):
    imgs = torch.stack([item[0] for item in batch], dim=0)
    bboxes = [item[1] for item in batch]
    clss = [item[2] for item in batch]
    return imgs, bboxes, clss

# %% [markdown]
# Net:

# %% [code]
def conv(in_channels, out_channels, kernel_size, stride=None, padding=None, shapeUnchange=False):
    if shapeUnchange:
        stride = 1
        padding = kernel_size // 2  # 3->1, 1->0
    return nn.Sequential(
        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),
        nn.ReLU(),
    )

class L2Norm(nn.Module):
    def __init__(self, n_channels, scale):
        super(L2Norm, self).__init__()
        self.n_channels = n_channels
        self.gamma = scale or None
        self.eps = 1e-10
        self.weight = nn.Parameter(apply_cuda(torch.Tensor(self.n_channels)))
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.constant_(self.weight, self.gamma)

    def forward(self, x):
        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt() # shape[b,1,38,38]
        x = x / norm   # shape[b,512,38,38]
        out = self.weight[None,...,None,None] * x
        return out

class Net(nn.Module):
    def __init__(self):
        super().__init__()
        layers = torchvision.models.vgg16(pretrained=True).features
        layers[16] = nn.MaxPool2d(2, ceil_mode=True)
        layers[-1] = nn.MaxPool2d(3, 1, 1)
        # from inplace=True to inplace=False
        layers[1], layers[3], layers[6], layers[8], layers[11], layers[13], layers[15], layers[18], layers[20], layers[22], layers[25], layers[27], layers[29] = [nn.ReLU() for i in range(13)]
        layers.add_module('31', nn.Conv2d(512, 1024, 3, padding=6, dilation=6))
        layers.add_module('32', nn.ReLU())
        layers.add_module('33', nn.Conv2d(1024, 1024, 1))
        layers.add_module('34', nn.ReLU())
        layers.add_module('35', nn.Conv2d(1024, 256, 1))
        layers.add_module('36', nn.Conv2d(256, 512, 3, stride=2, padding=1))
        layers.add_module('37', nn.Conv2d(512, 128, 1))
        layers.add_module('38', nn.Conv2d(128, 256, 3, stride=2, padding=1))
        layers.add_module('39', nn.Conv2d(256, 128, 1))
        layers.add_module('40', nn.Conv2d(128, 256, 3))
        layers.add_module('41', nn.Conv2d(256, 128, 1))
        layers.add_module('42', nn.Conv2d(128, 256, 3))
        self.layers = layers

        self.kernel = [21, 33, 36, 38, 40, 42]

        self.CLS, self.OFFSET = nn.Sequential(), nn.Sequential()  # 用list pytorch无法识别
        for i, v in enumerate(self.kernel):
            self.CLS.add_module(f'{i}', nn.Conv2d(self.layers[v].out_channels, num_anchors[i] * num_cls, 3, padding=1))
            self.OFFSET.add_module(f'{i}', nn.Conv2d(self.layers[v].out_channels, num_anchors[i] * 4, 3, padding=1))

        self.anchors = []
    
        shape = torch.zeros((1, 3, input_edge, input_edge))
        nw = 0
        for i, v in enumerate(self.kernel):
            for j in range(nw, v+1):
                shape = self.layers[j](shape)
            nw = v+1
            anchor = getAnchors(shape.shape[3], shape.shape[2], sizes[i], ratios[i]) # 先3后2
            self.anchors.append(anchor)
        self.anchors = torch.cat(self.anchors, dim=0)

        self.norm = L2Norm(self.layers[self.kernel[0]].out_channels, 20)
        
    
    def frozen(self, kd):
        req_grad = not kd
        for i, l in enumerate(self.layers.children()):
            if i > 30: break
            for p in l.parameters():
                p.requires_grad = req_grad
                

    def forward(self, x):
        def transform(x, t):  # (#batch, #anchor*t, w, h) -> (#batch, w*h*#anchor, t)
            return x.permute(0, 2, 3, 1).reshape((x.shape[0], -1, t))

        cls_preds, offset_preds = [], []
        nw = 0
        for i, v in enumerate(self.kernel):
            for j in range(nw, v+1):
                x = self.layers[j](x)
            nw = v+1
            y = x if i != 0 else self.norm(x)
            cls_preds.append(transform(self.CLS[i](y), num_cls))
            offset_preds.append(transform(self.OFFSET[i](y), 4))

        cls_preds = torch.cat(cls_preds, dim=1)
        offset_preds = torch.cat(offset_preds, dim=1)

        return self.anchors, cls_preds, offset_preds

# %% [markdown]
# Loss:

# %% [code]
def log_sum_exp(x):
    x_max = x.data.max()
    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max

class MultiBoxLoss(nn.Module):
    def __init__(self):
        super().__init__()
        self.neg_ratio = lambda x : x * 3
        self.alpha = 1

    def forward(self, cls_preds, cls_gts, offset_preds, offset_gts): # todo ????????????????????

        num = cls_preds.shape[0]
        pos = cls_gts != 0
        num_pos = pos.long().sum(dim=1)
        
        N = num_pos.sum()
        if N == 0:
            return apply_cuda(torch.tensor(0., requires_grad=True))

        loss_l = nn.SmoothL1Loss(reduction='sum')(offset_preds[pos], offset_gts[pos])  # 预测偏置值

        batch_size = cls_preds.shape[0]
        cls_preds = nn.Softmax(dim=1)(cls_preds.reshape(-1, num_cls)).reshape(batch_size, -1, num_cls)

        loss = -torch.log(torch.gather(cls_preds, 2, cls_gts.unsqueeze(2)).squeeze(2))
        loss[cls_gts==15] *= 0.4 # 人太多的, 降低一下权重
        #( -log[cls_preds[i][j][cls_gts[i][j]]])
        
        loss_c = apply_cuda(torch.tensor(0., requires_grad=True))
        for i in range(batch_size):
            if (num_pos[i] == 0): continue
            loss_c = loss_c + torch.sum(loss[i][pos[i]]) # 正样本
            lis = loss[i][~pos[i]] # 负样本
            _, idx = lis.sort(descending=True)
            _, rk = idx.sort()
            num_neg = torch.clamp(self.neg_ratio(num_pos[i]), max=lis.shape[0])
            loss_c = loss_c + torch.sum(lis[rk < num_neg])
                        
        return (loss_c + self.alpha * loss_l) / N

# %% [markdown]
# train

# %% [code]
if True:
    batch_size = 20
    kind = "train-easy" # todo difficult
    dataset = VOC_Train_Dataset("train-easy")
    loader = Data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)

    model = apply_cuda(Net())
    params_path = root/"input"/"ssd-pascal-voc-2012-dataset"/"params.ckpt"
    output_path = root/"working"/"params.ckpt"
    if params_path.exists() and True:
        model.load_state_dict(torch.load(str(params_path)))

    frozen = False # 先用vgg的现有预训练模型作为骨干, 冻结骨干练, 再整个练
    model.frozen(frozen)
    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)
    criter = MultiBoxLoss()

    losy = []
    plt.ion()
    for epoch in range(10):
        lsum = 0
        tot = 0
        for imgs, gt_bboxs, gt_clss in loader:
            anchors, cls_preds, offset_preds = model(imgs)
            cls_gts, offset_gts = match(anchors, gt_bboxs, gt_clss)
            loss = criter(cls_preds, cls_gts, offset_preds, offset_gts)
            lsum += loss
            optim.zero_grad()
            loss.backward()
            optim.step()

            tot += batch_size
            print(f"\repoch{epoch}: {tot} / {len(dataset)}, total lost = {lsum / tot}", end=' ')
            
            for i in range(0, 1):
                cls_pred = nn.Softmax(dim=1)(cls_preds[i]).argmax(dim=1)
                pos = cls_pred > 0
                print(pos.long().sum(), end=' ')
                print((cls_gts[i]>0).long().sum(), end=' ')
            
            if tot % 1000 == 0:
                torch.save(model.state_dict(), output_path)
            if tot % 5000 == 0:
                for i in range(0, 2):
                    img = imgs[i].permute(1, 2, 0).cpu().numpy() # tensor 转 cv2
                    cls_pred = nn.Softmax(dim=1)(cls_preds[i]).permute(1, 0) # 先种类, 再prior
                    boxs = decode(anchors, offset_preds[i]) # box 还原
                    boxs, cls_pred = Detect(cls_pred, boxs)
                    boxs *= input_edge
                    plt.imshow(img)
                    plt.pause(1)
                    Image(img).visualize(boxs, cls_pred)
                    plt.pause(1)
                    '''
                    img = imgs[i].permute(1, 2, 0).cpu().numpy() # tensor 转 cv2
                    cls_pred = nn.Softmax(dim=1)(cls_preds[i])
                    chose = torch.max(cls_pred, dim=1)
                    mask = (chose[0] > 0.5) & (chose[1] > 0)
                    boxs = decode(anchors[mask], offset_preds[i][mask]) # box 还原
                    boxs *= input_edge
                    cls_pred = torch.argmax(cls_pred, dim=1)[mask]
                    plt.imshow(img)
                    plt.pause(1)
                    Image(img).visualize(boxs, cls_pred)
                    plt.pause(1)
                    '''
        losy.append(lsum)
        plt.cla()
        plt.plot(list(range(epoch + 1)), losy, marker='x')
        plt.pause(1)
        torch.save(model.state_dict(), output_path)

    plt.ioff()
    plt.show()

# %% [markdown]
# detector

# %% [code]
if False:
    batch_size = 10
    kind = "val-easy" # todo difficult
    dataset = VOC_Train_Dataset("val-easy")
    loader = Data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)

    model = apply_cuda(Net().eval())
    params_path = root/"input"/"ssd-pascal-voc-2012-dataset"/"params.ckpt"
    if params_path.exists():
        model.load_state_dict(torch.load(str(params_path)))

    with torch.no_grad():
        for imgs, gt_bboxs, gt_clss in loader:
            anchors, cls_preds, offset_preds = model(imgs)
            for i in range(batch_size): 
                img = imgs[i].permute(1, 2, 0).cpu().numpy() # tensor 转 cv2
                cls_pred = nn.Softmax(dim=1)(cls_preds[i]).permute(1, 0) # 先种类, 再prior
                boxs = decode(anchors, offset_preds[i]) # box 还原
                boxs, cls_pred = Detect(cls_pred, boxs)
                boxs *= input_edge
                plt.imshow(img)
                plt.pause(1)
                Image(img).visualize(boxs, cls_pred)
                plt.pause(1)
            break

# %% [code]
if False: #test
    batch_size = 20
    kind = "train-easy" # todo difficult
    dataset = VOC_Train_Dataset("train-easy")
    loader = Data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    
    model = apply_cuda(Net())
    params_path = root/"input"/"ssd-pascal-voc-2012-dataset"/"params.ckpt"
    if params_path.exists():
        model.load_state_dict(torch.load(str(params_path)))
        
    anchors = model.anchors
    for imgs, gt_bboxs, gt_clss in loader:
        cls_gts, offset_gts = match(anchors, gt_bboxs, gt_clss)
        
        for i in range(len(gt_bboxs)):
            img = imgs[i].permute(1, 2, 0).cpu().numpy()
            pos = cls_gts[i] > 0
            #boxs = decode(anchors[pos], offset_gts[i][pos])
            boxs = anchors[pos]
            boxs *= input_edge
            plt.imshow(img)
            plt.show()
            Image(img).visualize(boxs, cls_gts[i][pos])
            plt.show()
        break
```