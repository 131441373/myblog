conditional gan

# supervised
给定条件向量c, 和另一个噪音向量z拼接作为gan的输入

## InfoGAN
为了让模型认识到c, G后接classfier分类c, 组成autoencoder, classifier和D要share部分参数

## VAE-GAN
x通过encoder到z, z通过decoder(G)到x', x'过D判断是否真实; 噪音z'通过G(decoder)到x'', x''过D判断是否真实
即encoder->decoder->D的结构, 前两组成VAE, 后两组成GAN
优化VAE部分(其他froze): x和x'来个L2, z要和噪音生成的normal distribution KL
优化GAN: 1)优化G: 让Dis(x'), Dis(x'')大 2)优化D: 让Dis(x)大, Dis(x'), Dis(x'')小

## BiGAN
x通过encoder到z; 噪音z'通过decoder到x'
(x, z)和(x', z')两个二元组分别丢给D, 让他判断来自enc还是dec
优化D: 给enc二元组高分, dec二元组低分
优化encoder: 让D给二元组低分
优化decoder: 让D给二元组高分

# unsupervised
如 style transfer, 语音男女生切换

## 法1
转换前gen成转换后然后dis (比如相当于gan的noise变成了一张图

额外还要求转换前后相似:
1)当gen很浅时, 本身就改不了很多

2)用pretrain的一个encoder, 对转换前后的结果都用一次, 要求他们两个不差太多

3)cycle gan: 同时训练gen x->y 和 gen y->x, 在原来基础上, 将结果再用gen转回去, 要求他们两越接近越好. 
为了训练gen y->x , 还要再弄另一个dis x, 同理训练, 然后同样也要 y->x->y 后 尽可能相近
(机器可能把信息隐藏得比较深, 但gen回来的时候gen又比较厉害能重新发现, 满足了转回去之后尽可能相近, 却不符合希望风格转换前后相关的需求)
(这个东西和 dual gan 和 disco gan 是一个东西)

4)star gan: 多方转换的模型(todo)

##法2
先encoder成features, 再decoder

```
x -> ENx \   / DEx -> x -> Dis x
           f
y -> ENy /   \ DEy -> y -> Dis y
```
训练时可以x->x, y->y reconstruct, 尽可能小error, 同时要求Dis的loss也小
但是这样训练本质上训练出来了两个VAE(VAEgan), 两者的f并没有什么相似性可言

1) couple gan
Enx和Eny在前面的大部分特征提取层独立训练, 最后几层的参数是共用的
同理DEx, DEy在前面几层参数共用, 后面独立训练

2) 加一个domain的dis
使得Enx和Eny都能骗过domain检测, 这样Enx就像Eny的结果了

3) cycle
x->f->y->f->x跑一大圈, 比较
或
x->f->y->f 比较两个f, 这个能更好的在features上去比较semantic