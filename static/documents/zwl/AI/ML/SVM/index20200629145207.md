### 线性可分

classification中
$log (1-g(z)), log(g(z))$ 换为分段直线拟合

使得最终的分类问题, 划分的间隔更大些, 更稳健(例如 12   89在5处切开, 而不是在7处切开或在3处切开

又称大间隔分类器

在最优化问题中, 把regularization中的lambda系数除掉 放在前面cost那变成一个C,问题等价
即
$J(\theta) = \frac 1 m \sum_{i=1}^m [ -y\ln(h_{\theta}(x)) -(1-y)\ln(1-h_{\theta}(x))] + \frac{\lambda}{2m} \sum_{i=1}^n \theta_i^2$
改为
$J(\theta) = C \sum_{i=1}^m [ -y\ln(h_{\theta}(x)) -(1-y)\ln(1-h_{\theta}(x))] + \frac 1 2 \sum_{i=1}^n \theta_i^2$
C决定了对噪音点的敏感程度, 如果C特别大, 可能一个噪音点就会打破原来的大间隔, 换一条不那么适合的线
所以C要大, 但不能过大

因为$h_\theta(x) = g(x \theta)$
在实际$y^{(i)}=1$时,  $x^{(i)} \theta$需$\gt ~~1$ ,  使C对应的cost为0
在实际$y^{(i)}=0$时,  $x^{(i)} \theta$需$\lt -1$, 使C对应的cost为0
(**当然, 这些条件不一定能全部同时满足, 但是C较大时, 这些条件是能够尽可能满足的**)

考虑 $x \theta$ 的几何意义, 由于$x$是横向的, $\theta$是竖向的, 矩阵乘法几何意义就是两者的点乘

根据点乘结果来划分,  得到的划分边界(decision boundary)就会是: 与$\theta$垂直的线(面)
**换而言之, $\theta$是划分边界的法向量**

此时, 最优化问题转为了$\frac 1 2 || \theta ||$ 在上述条件下的条件极值
要使法向量尽可能短就能满足上述条件, 相当于尽可能使各个$x$投影到法向量上尽量长
如图(假设边界过原点)

![](https://img2020.cnblogs.com/blog/1086046/202006/1086046-20200629105826669-153203.png)

### 线性不可分
类似酉变换的, 定义核变换为 $u\cdot v := u^T v = v^T u$ 内积定义下, 满足 $<u, v> = <Tu, Tv>$ 的变换 $T$
