需要额外download词库
看[官网的manual install](https://www.nltk.org/data.html) 或github，自己下载包，设置环境变量
或 https://pan.baidu.com/share/init?surl=oUsf-FgVAZnQAtZWRwiK4w (9sor

### 一个好教程

`https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908`
后面有关于 引用代词替换 , 词组提取, 人物/公司关系提取 等高级技巧

### 分词 (tokenize
#### 分词器 (tokenizer)
> `nltk.tokenize.WhitespaceTokenizer()` 按空格分

> `nltk.tokenize.WordPunctTokenizer()` 按punctuation分

> `nltk.tokenize.TreebankWordTokenizer()` 按照英文语法 (√) 比如 isn't 分成 is 和 n't, Mrs.可以识别, 网址不能正常划分

> `nltk.tokenize.casual.TweetTokenizer()` 正如其名, 更casual, 更多奇葩token可以识别 (√) 但是isn't不能划分, Mrs.不能识别; 网址, emoji可以有效划分

使用 `tokenizer = 上述中的一个`
然后 `tokenizer.tokenize(text)` 即可
要多多个text组成的list用类似map的功能可以用 `tokenizer.tokenize_sents(text_list)` 

这些个tokenizer都是assume已经分好句了的
分句用 `nltk.sent_tokenize()` 

### 词性标注
`tags = nltk.pos_tag(tokens)` 获得一个list, 里面每个元素是tuple(token, tag)
`nltk.pos_tag_sents((tokens1, tokens2, tokens3, ...))` 多个句子的token一起操作 (方便了我们不用自己写map函数

词性表 : https://blog.csdn.net/JasonJarvan/article/details/79955664

### 命名实体识别
`nltk.ne_chunk(tags)`
返回的是一个`nltk.tree.tree`类型 (todo

### 小写化
除了命名实体, 其他小写化
直接用 python str 自带的 `.lower()`

### 词干提取 (stemming) 或 词形还原 (lemmatize)
#### 词形还原器 (lemmatizer)

> `nltk.stem.WordNetLemmatizer()` 词形还原

```
tokenizer = nltk.tokenize.TreebankWordTokenizer()
stemmer = nltk.stem.WordNetLemmatizer()
def get_pos(tag):
    if tag.startswith('J'): return wordnet.ADJ
    elif tag.startswith('V'): return wordnet.VERB
    elif tag.startswith('R'): return wordnet.ADV
    else: return wordnet.NOUN # 剩下的不是noun也写noun没问题的, 看了好多网站都这么写

words = tokenizer.tokenize(str)
tags = nltk.pos_tag(words)
for i, (w, tag) in enumerate(tags):
    words[i] = stemmer.lemmatize(w, pos=get_pos(tag)) # 针对性的词形还原, 否则效果不好
```

#### 词干提取器 (stemmer)
> `nltk.stem.PorterStemmer()` 使用广泛, 提取温柔, 古老算法, 偏慢

> `nltk.stem.SnowballStemmer('english')` porter改进版, 比porter稍快 (√)

>`nltk.stem.LancasterStemmer` 超级激进, 甚至导致错误, 算法偏快,  不推荐

使用 `stemmer = 上述中的一个`
然后 `stemmer.stem(text)` 即可

#### todo
是否先 lemmatize 再 stemmer 有效
还是说单独使用, 那么哪个有效


### ban掉 停用词 和 标点符号
停用词(is, and等超常用)

```
ban = set.union(set(nltk.corpus.stopwords.words('english')), set(string.punctuation))
words = [w for w in words if not w in ban] # 停用词 和 标点符号
```

### 词性转换
```

lemmatizer.lemmatize("caring", pos='v') # 转为动词 care
```
