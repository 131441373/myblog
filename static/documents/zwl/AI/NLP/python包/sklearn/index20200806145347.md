### feature_extraction

#### Count
相当于就是统计了frequency
使用方法与下者同, 这里不赘述

#### TF-IDF
[doc](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)
```
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

tfidf = TfidfVectorizer(min_df=2, max_df=0.7, ngram_range=(1, 2), sublinear_tf=True)
# min_df, max_df 表示的是 data_frequency的上下界, 使用整数表示次数, 使用0~1的小数则表示百分比
# max_df 相当于起停用词表的作用, min_df 相当于筛掉那些极有可能是typo的词
# ngram_range=(1, 2) unigram和bigram同时使用
# sublinear_tf 对 tf 使用 1+log f 方法
features = tfidf.fit_transform(texts) # texts是若干字符串, 每个字符串视为一个文档
# features print出来是若干行: (第几个文档, 第几个gram) tfidf权值
pd.DataFrame(
    features.todense(), # todense获得一个矩阵, 每行是一个document的tfidf向量表示, 向量有#gram维, 第i维的值为该文档中第i个gram的权值
    columns=tfidf.get_feature_names() # 前面fit的时候把各个gram对应的文本赋值给了Vectorizer, 可以用这种方式获取
)
```

针对已经token好的文本如 `[['I', 'love'], ['love', 'you']]`
设置以下两个参数
`tfidf = sktext.TfidfVectorizer(tokenizer=lambda x:x, lowercase=False)`