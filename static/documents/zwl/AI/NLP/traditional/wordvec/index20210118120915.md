
# model
## continous bag of word (CBOW)
context word 的 one-hot 分别嵌入后取平均，然后做classification希望分类为center word

input->hidden获取的是context word的embedding
分类器每个点的入边拼成向量v在神经网络中本质上是与嵌入向量u点乘
所以output->hidden获取的是center word的embedding

## skip-gram
center word 的 one-hot 嵌入后，做classification得context word（位置无关）的probability distribution
评估时假装context word互相独立，概率为各临近词的概率乘积

input->hidden获取的是center word的embedding
同理上
output->hidden获取的是context word的embedding

# 优化
## negative sampling
没有说很确切的gt，所以不能像图片的negative sampling那样挑选错误分辨但认为很对的那种来求loss
只能说通过随机，或者平滑处理的随机，去sample一些样本

## hierarchy softmax （softmax维度太大）
CBOW中
弄个哈夫曼树， 词在叶子，概率为从上到下随机游走概率
每个节点有个可训练的参数vector $\theta$， 根据其与context嵌入向量点乘取sigmoid， 作为该节点往左走的概率（向右走=1-向左）

