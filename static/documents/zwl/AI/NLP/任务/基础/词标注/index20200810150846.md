
$t = (t_1 \cdots t_n)$ 为标注
$w = (w_1 \cdots w_n)$ 为词

$\hat t = \argmax_t p(t|w) = \argmax_t \frac {p(w | t) p(t) }  p(w) = \argmax_t p(w | t) p(t)$

## HMM (hidden markov model)
定义详见概率图模型那里
可以理解为 词序列 是由 某个 tag序列生成的

V种词的显性观测序列(w) 和 M种标签的隐性状态序列
状态值满足markov: $p(t) = \prod_i p(t_i | t_{i-n+1}^{i-1})$
观测独立性: $p(w | t) = \prod_i p(w_i | t_i)$
希望解决序列问题:  给定HMM模型u和观测序列O,  从该观测序列反推出最优的状态序列Q, 即最大化 $P(Q|O,u)$

u的markov可以用训练集, 类似语言模型ngram那样训练
状态->观测这个也可以用训练集非常容易的完成
或者用 HMM经典问题3? (todo)

## BiLSTM
BiLSTM适合用于建模上下文信息(双向), 但不适合做生成(生成单向)

## 模板 (BiLSTM识别NER)
```
# %% [code]
import pandas as pd

from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
import torch.utils.data as Data

from pathlib import Path

import matplotlib.pyplot as plt
plt.style.use("seaborn-paper")

# %% [code]
def apply_cuda(*args):
    if torch.cuda.is_available():
        args = tuple(map(lambda x: x.cuda(), args))
    return args if len(args) > 1 else args[0]

# %% [code]
root = Path().resolve().parent # 本地的话用 Path(__file__)

# %% [markdown]
# ## Load Data

# %% [code]
df = pd.read_csv("../input/ner.csv", encoding='ISO-8859-1', error_bad_lines=False) # 数据问题
df = df[['sentence_idx','word','tag']]
df.dropna(inplace=True)

sentences = df.groupby("sentence_idx").apply(lambda df: [(w, t) for w,t in zip(df["word"].values.tolist(), df["tag"].values.tolist())]).values.tolist()

# %% [code]
words = list(set(df["word"].values))
words += ['<s>', '</s>']
n_words = len(words)

tags = list(set(df["tag"].values))
n_tags = len(tags)

# %% [markdown]
# ## map String to Number

# %% [code]
word2idx = {w: i for i, w in enumerate(words)}
tag2idx = {t: i for i, t in enumerate(tags)}

sentences = [
    [(word2idx['<s>'], tag2idx['O'])] + [(word2idx[w], tag2idx[t]) for w, t in s]
    for s in sentences
]

# %% [markdown]
# ## EDA

# %% [markdown]
# #### Length Distrubution

# %% [code]
plt.hist([len(s) for s in sentences], bins=50)
plt.show()

# %% [markdown]
# *Observation : pad_sequence to length about 50*

# %% [code]
print(f"number of words : {n_words}")
print(f"number of tags : {n_tags}")

# %% [markdown]
# ## Text Preprocessing

# %% [code]
maxlen = 50 # from above EDA observation

# %% [code]
X = [[w for w, _ in s] for s in sentences]
X = pad_sequences(maxlen=maxlen, sequences=X, padding="post", truncating="post", value = word2idx['</s>'])
Y = [[t for _, t in s] for s in sentences]
Y = pad_sequences(maxlen=maxlen, sequences=Y, padding="post", truncating="post", value = tag2idx["O"])

# %% [markdown]
# ## Define Model

# %% [code]
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.emb = nn.Embedding(n_words, 500)
        self.rnn = nn.LSTM(input_size=500, hidden_size=300, batch_first=True, bidirectional=True)
        self.cls = nn.Linear(300*2, n_tags)
        
    def forward(self, x):
        x = self.emb(x)
        x,_ = self.rnn(x)
        x = self.cls(x)
        return x

# %% [markdown]
# ## Define Dataset

# %% [code]
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)

# %% [code]
class NER_Dataset(Data.Dataset):
    def __init__(self, kind):
        self.X = X_train if kind == 'train' else X_test
        self.Y = Y_train if kind == 'train' else Y_test
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return apply_cuda(torch.LongTensor(self.X[idx]), torch.LongTensor(self.Y[idx]))

# %% [markdown]
# ## Train

# %% [code]
if True:
    batch_size = 20
    dataset = NER_Dataset("train")
    loader = Data.DataLoader(dataset = dataset, shuffle = True, batch_size = 20)
    
    model = apply_cuda(Net())
    params_path = root/"working"/"params.ckpt"
    output_path = root/"working"/"params.ckpt"
    if params_path.exists() and True:
        model.load_state_dict(torch.load(str(params_path)))
        
    optim = torch.optim.Adam(model.parameters(), lr=1e-4)
    criter = nn.CrossEntropyLoss()
    
    losy = []
    for epoch in range(10):
        lsum = 0
        tot = 0
        for X, T in loader:
            Y = model(X)
            loss = criter(Y.transpose(1, 2), T)
            lsum += loss
            optim.zero_grad()
            loss.backward()
            optim.step()
            tot += batch_size
            print(f"\repoch{epoch}: {tot} / {len(dataset)}, total lost = {lsum / tot}", end=' ')
            if tot % 5000 == 0:
                torch.save(model.state_dict(), output_path)
        losy.append(lsum)
        plt.plot(list(range(epoch + 1)), losy, marker='x')
        plt.show()
        torch.save(model.state_dict(), output_path)

# %% [code]
if True:
    batch_size = 20
    dataset = NER_Dataset("test")
    loader = Data.DataLoader(dataset = dataset, shuffle = True, batch_size = 20)
    
    model = apply_cuda(Net())
    params_path = root/"working"/"params.ckpt"
    if params_path.exists() and True:
        model.load_state_dict(torch.load(str(params_path)))
    
    err = 0
    num = 0
    for X, T in loader:
        Y = model(X)
        Y = torch.argmax(Y, dim=-1)
        err += torch.sum(Y!=T)
        num += torch.sum(T!=tag2idx['O'])
        
    err /= len(dataset)
    num /= len(dataset)
    
    print(f"average err: {err}")
    print(f"average num: {num}")
```