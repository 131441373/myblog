简称LM

## n元语法ngram
按照条件概率公式

$$P(w_1\cdots w_n) = \prod_{i=1}^n P(w_i | w_1 \cdots w_{i-1})$$

但是这个计算方法太复杂了
所以改用markov n元语法
定义$S_n = (w_1 \cdots w_n)$
约定$w_{2-n}$到$w_0$为<s>表示句首, $w_{n+1}$为</s>表示句尾 (S表示sentense)

$$p(S_n) \approx \prod_{i=1}^n p(w_i | w_{i-n+1}^{i-1})$$

**避免乘法, 可以把后面的连乘改成log连加**

注意n元语法的vocabulary要把 <s> 和 </s> 也算上

实际应用中n一般为2/3

缺点: 语言的依赖性很强
例如`the computer [which i had just put on the fifth floor] crashed` 这个crashed 并不是修视floor的, 这是多少元语法都不能解决的缺陷

### 最大似然估计
对训练集, 定义$c(s)$表示s作为子串在训练集中的出现次数
最大似然估计 将 $p(w_i | w_{i-n+1}^{i-1})$　定义为  $\frac {c(w_{i-n+1}^i)}{c(w_{i-n+1}^{i-1})}$
可以理解为: 从大量的阅读中, 提取出某些语法, 如哪些是固定搭配,  那些词不会在附近一起用之类的
于是

$$p(S_n) = \prod_{i=1}^n \frac {c(w_{i-n+1}^i)}{c(w_{i-n+1}^{i-1})}$$

缺点: 语料库总是不够大的($V^N$ >> 语料库中的n元组)
如果某个分子$c(w_{i-n+1}^i)$ 为0, 在连乘机制下, 整个计算结果就是0了
例如 3元语法中: go to school, go to swim, go to picnic.... 总可能会有某个 go to xxx 不在训练集中

### 模型平滑化处理

平滑处理的基本思想是劫富济贫

#### 加d法 (laplace平滑)
假设每个ngram出现多 $\delta$ 次 ($\delta\in [0, 1]$

分母有个V是因为给定 n-1前缀, 其对应n个ngram都出现多了$\delta$次

$$p(S_n) = \prod_{i=1}^n \frac {\delta + c(w_{i-n+1}^i)}{\delta \times V + c(w_{i-n+1}^{i-1})}$$

缺点: 济贫毫无策略, 无比平均

应用于: 一些0没有那么多的任务

#### Good-Turing法
记 $n_r$ 表示 $c(s)=r$ 的ngram有多少种

将 $c(s)=r$ 的串, 定义 $c^*(s) = r^* = (r+1)\frac {n_r+1}{n_r}$ 

将原算法的分子改为 $c^*(s)$ , 分母不变, 就提供了对未出现ngram的扶贫资金
出现0次的串分子也按上式算 $c^*(s) = \frac {n_1} {n_0}$ (平均分配这些扶贫资金

这就满足 $\sum_{ngram} c^*(ngram) = \sum_{r=0}^{\infty} n_r * (r+1)\frac {n_r+1}{n_r} = \sum_{r=1}^{\infty} r * n_r = 分母$

缺点: 由于 $n_r=0$ 会直接失效, 这个算法只是提供思路
同时扶贫资金的平均分配政策也不合理

#### 插值法interpolation (kneser-ney)
http://smithamilli.com/blog/kneser-ney/

根据good-turing的计算结果,  发现对于非常多的 $r$ ($r\ge 3$左右), $r$ 和 $r^*$ 的关系就是 $r^* \approx r - d$
其中 $d\le 1$ 为一个常数, 据说比较优的设置为 $\frac {n_1}{n_1 + 2 n_2}$ (统计自然语言处理)

于是对于分子$c(s)>0$, 令为 $c^*(s) = c(s) - d$ (称为absolute discount法)
这样就多了扶贫基金 
由于 $c(s)=0$ 的 $c^*$ 仍为 $0$, 所以两者合在一起简记为 $\max \lbrace c(s) - d, 0\rbrace$

定义 $N_+(s \cdot) =$  #{ $w ~|~ c(s w) > 0$ } 即 历史后能接多少种不同的单词
$N_+(\cdot s)$ 之类的同理, 只是在哪个位置加字幕的区别

则从 $N_+ (w_{i-n+1}^{i-1} \cdot)$ 个 ngram 中夺得了扶贫基金共 $\lambda_n = N_+ (w_{i-n+1}^{i-1} \cdot) \frac d {c(w_{i-n+1}^{i-1})}$

按照某个概率和为1的概率分布把他们分了就好
此时有两种方法, 一种是backoff类的, 即c是0才分,  一种是interpolation类的, 前面收了税的也给你一点, 这里采用后者

第一层: 

$$P(w_i|w_{i-n+1}^{i-1}) = \frac {\max \lbrace c(w_{i-n+1}^{i}) - d, 0) \rbrace} {c(w_{i-n+1}^{i-1})} + \lambda_n P(w_i | w_{i-n+2}^{i-1})$$

第$k\gt 1$层:

$$P(w_i|w_{i-n+k}^{i-1}) = \frac {\max \lbrace N_+(\cdot w_{i-n+k}^i) - d, 0 \rbrace} {N_+(\cdot w_{i-n+k}^{i-1} \cdot)} + \lambda_{n-k+1} P(w_i | w_{i-n+(k+1)}^{i-1})$$

最后一层:

$$P(w_i) = \frac {N_+(\cdot w)}{N_+(\cdot \cdot)}$$

其中第二层开始, 本质上就是考察新加词对前面各个词的关联度

优势: 这个方法在一般情形都比backoff有效

##### 修正
上述kneser-ney使用的d在 $r \ge 3$ 才适用, 所以针对性的进行了一些修正
https://core.ac.uk/download/pdf/22877567.pdf

$D_i = i - \frac {(i+1) N_{i+1}}{N_i} d, i \le 3$
$D_i = D_i, i \gt 3$
相应的扶贫基金数量也要改

### stupid back-off (katz
对于w_i, 如果trigram不是0就直接用, 否则改用bigram并乘一定系数(0.4), 依次类推

优势: 对于超级无敌巨大的corpus (web等) , 这个方法相对interpolation有效

### LM评判

#### 法1 : 放到具体任务中
如 机器翻译, 语音识别, 拼写检查

缺点:难操作 + 耗时

#### 法2 : 困惑度 perplexity(PP)
基本思想是：给测试集的句子赋予较高概率值的语言模型较好

由于概率是连乘, 所以用$\sqrt[n]{~}$ 来normalize
再根据困惑度的语义, 希望概率越高, 困惑度越小, 所以再取个倒数

$$PP(S_n) = P(S_n)^{-\frac 1 n} = 2 ^ {\frac 1 n \sum_{i=1}^n -log_2 P(S_n)}$$

本质就是熵越小越好


### 其他

#### 文段

对于多个句子组成的文段 $T = (S_1, \cdots, S_m)$, 视为各句(用标点符号分隔开)之间有若干EOS和BOS

$$p(T) = \prod_{i=1}^m p(S_i)$$
#### 生词
<UNK> token (unknown

## skip-gram