spam类 / 情感分析 / 作者性别判断 ...

给定文档, 和一系列类别, 求分类

### SVM
直接将词向量相加, 然后跑SVM

### Text-CNN
conv1d (实际还是二维: 句子的顺序一维, 不同的token一维, 但是filter的框的长就是token的个数, 所以filter只向句子顺序方向移动,所以叫1d)

这时不同句子得到的向量长度不同, 所以再加一个 AdaptiveMaxpool1d 或 GlobalMaxpool1d

### BiLSTM
使用BiLSTM模型

### Sentiment Analysis 模板 (IMDB database
```
# %% [code]
import numpy as np
import pandas as pd

from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
import torch.utils.data as Data

from pathlib import Path

import matplotlib.pyplot as plt
plt.style.use("seaborn-paper")

# %% [code]
def apply_cuda(*args):
    if torch.cuda.is_available():
        args = tuple(map(lambda x: x.cuda(), args))
    return args if len(args) > 1 else args[0]

# %% [code]
root = Path().resolve().parent

# %% [markdown]
# ## Load Data

# %% [code]
df = pd.read_csv("../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv")
sentiments = df['sentiment'].values.tolist()

# %% [code]
tokens = np.load("../input/text-preprocessing-imdb-review-dataset/tokens.npy", allow_pickle=True)
sentences = tokens.tolist()

# %% [code]
words = list(set.union(*[set(s) for s in sentences]))
words += ['<s>', '</s>']
n_words = len(words)
tags = list(set(sentiments))
n_tags = len(tags)

# %% [code]
word2idx = {w: i for i, w in enumerate(words)}
tag2idx = {t: i for i, t in enumerate(tags)}
sentences = [
    [word2idx['<s>']] + [word2idx[w] for w in s] 
    for s in sentences
]

# %% [markdown]
# ## EDA

# %% [code]
plt.hist([len(s) for s in sentences], bins=50)
plt.show()

# %% [markdown]
# *Observation : pad_sequence to length about 150*

# %% [code]
print(f"number of words : {n_words}")
print(f"number of tags : {n_tags}")

# %% [markdown]
# ## Text PreProcessing

# %% [code]
maxlen = 150 # from above EDA observation

# %% [code]
X = [[w for w in s] for s in sentences]
X = pad_sequences(maxlen=maxlen, sequences=X, padding="post", truncating="post", value = word2idx['</s>'])
Y = [tag2idx[s] for s in sentiments]

# %% [markdown]
# ## Net

# %% [code]
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.emb = nn.Embedding(n_words, 400)
        self.rnn = nn.LSTM(input_size=400, hidden_size=200, batch_first=True, bidirectional=True)
        self.cls = nn.Linear(200*2, 2)
        
    def forward(self, x):
        x = self.emb(x)
        _,(hn,_) = self.rnn(x)
        x = torch.cat((hn[0], hn[1]), dim=1) # (dir, batch, dim) -> (batch, dir*dim)
        x = self.cls(x)
        return x

# %% [markdown]
# ## Dataset

# %% [code]
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)

# %% [code]
class IMDB_Dataset(Data.Dataset):
    def __init__(self, kind):
        self.X = X_train if kind == 'train' else X_test
        self.Y = Y_train if kind == 'train' else Y_test
        
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return apply_cuda(torch.LongTensor(self.X[idx]), torch.tensor(self.Y[idx], dtype=torch.int64))

# %% [code]
if True:
    batch_size = 20
    dataset = IMDB_Dataset("train")
    loader = Data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)
    
    model = apply_cuda(Net())
    params_path = root/"working"/"params.ckpt"
    output_path = root/"working"/"params.ckpt"
    if params_path.exists() and True:
        model.load_state_dict(torch.load(str(params_path)))
        
    optim = torch.optim.Adam(model.parameters(), lr=1e-3)
    criter = nn.CrossEntropyLoss()
    losy = []
    for epoch in range(10):
        lsum = 0
        tot = 0
        for w, s in loader:
            p = model(w)
            loss = criter(p, s)
            lsum += loss
            optim.zero_grad()
            loss.backward()
            optim.step()
            tot += batch_size
            print(f"\repoch{epoch}: {tot} / {len(dataset)}, total lost = {lsum / tot}", end=' ')
            if tot % 5000 == 0:
                torch.save(model.state_dict(), output_path)
        losy.append(lsum)
        plt.plot(list(range(epoch + 1)), losy, marker='x')
        plt.show()
        torch.save(model.state_dict(), output_path)

# %% [code]
if True:
    batch_size = 20
    dataset = IMDB_Dataset("test")
    loader = Data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)
    
    model = apply_cuda(Net())
    params_path = root/"working"/"params.ckpt"
    if params_path.exists() and True:
        model.load_state_dict(torch.load(str(params_path)))
        
    for epoch in range(1):
        lsum = 0
        for w, s in loader:
            p = model(w)
            p = nn.Softmax(dim=1)(p)
            for b in range(batch_size):
                ps = 1 if p[b][1] > p[b][0] else 0
                if ps != s[b]: lsum += 1

    print(f'err rate:{lsum / len(dataset)}')
```