## 数据集
WMT
AI Challenger
UM-Corpus

## BLEU 评价标准
```
from nltk.translate.bleu_score import sentence_bleu
reference = [['this', 'is', 'small', 'test']] # 标准, 可以拥有多个
hypothesis = ['this', 'is', 'a', 'test'] # 预测
score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)) # 默认1234gram权重为(0.25, 0.25, 0.25, 0.25)
# 还有一个 corpus_bleu
```

https://www.cnblogs.com/by-dream/p/7679284.html

## Encoder + Decoder
语言到语言有 n*n 种转换器
但是 语言到隐藏表示, 隐藏表示到语言, 则只需要 n+n 种转换器

Encoder一般就是 RNN系列, 得到 len个h输出
Decoder一般也是 RNN也是RNN序列, 进行一个序列生成, 输入为生成序列的上一个词加上encoder中的一些信息

### Attention
类似人工做的时候的 Alignment: 将两种语言的单词对应关系找出——翻译目标语言的第i个词与被翻译语言的哪些词有关

Global attention: Encoder输出加权平均, 这个加权与Decoder生成到哪个词了有关
Local attention: 全局加权平均太耗时, 训练一个模型, 让它大致识别目标语言的每个词对应的相关词窗口, 在那个窗口加权平均

## 模板
```
# %% [code]
import numpy as np
import pandas as pd

from keras.preprocessing.sequence import pad_sequences

from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
import torch.utils.data as Data
from copy import deepcopy

from pathlib import Path

import matplotlib.pyplot as plt
plt.style.use("seaborn-paper")

# %% [code]
root = Path().resolve().parent

# %% [code]
def apply_cuda(*args):
    if torch.cuda.is_available():
        args = tuple(map(lambda x: x.cuda(), args))
    return args if len(args) > 1 else args[0]

# %% [markdown]
# ## Net

# %% [code]
def clones(x, times):
    return nn.ModuleList([deepcopy(x) for i in range(times)])

# %% [code]
def get_attn_pad_mask(seq_q, seq_k):
    return (seq_k == 0).unsqueeze(1).expand(seq_k.shape[0], seq_q.shape[1], seq_k.shape[1]) # 0 为 pad

def get_attn_subsequence_mask(seq):
    return apply_cuda(torch.triu(torch.ones( (seq.shape[0], seq.shape[1], seq.shape[1]) ), diagonal=1) == 1)

# %% [code]
class Embedding(nn.Module):
    maxlen = 100

    def __init__(self, input_dim, output_dim):
        super().__init__()
        # embedding
        self.emb = nn.Embedding(input_dim, output_dim)
        self.output_dim = output_dim

        # position encoding
        coef = torch.mm(
            torch.arange(0, self.maxlen).unsqueeze(1).float(),
            torch.exp(-1 * torch.arange(0, output_dim, 2) * (np.log(10000.0) / output_dim)).unsqueeze(0)
        )
        self.pe = apply_cuda(torch.zeros(self.maxlen, output_dim))
        self.pe[:, 0::2] = torch.sin(coef) # 偶数
        self.pe[:, 1::2] = torch.cos(coef) # 奇数

        self.norm = nn.LayerNorm(output_dim)
        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        x = self.emb(x) * np.sqrt(self.output_dim) + self.pe[:x.shape[1]] # todo: why mult sqrt dim
        return self.dropout( self.norm(x) ) # +pe自动补全batch维度

# %% [code]
class MultiHeadedSelfAttention(nn.Module):
    class MultiHead(nn.Module):
        def __init__(self, d_model, n_head):
            super().__init__()
            self.W = nn.Linear(d_model, d_model)  # 分n_head个矩阵乘 --优化为-> 乘完再拆
            self.n_head = n_head

        def forward(self, x):  # -> (batch, n_head, len, dim)
            return self.W(x).reshape(x.shape[0], x.shape[1], self.n_head, -1).transpose(1, 2)

    def __init__(self, d_model, n_head):
        super().__init__()
        self.dim = d_model // n_head
        self.n_head = n_head
        self.Ws = clones(self.MultiHead(d_model, n_head), 3)
        self.dropout = nn.Dropout(0.1)

    def forward(self, Q, K, V, mask=None):
        Q, K, V = (W(x) for W, x in zip(self.Ws, (Q, K, V))) # 转为多个head

        # (batch, n_head, len, d)Q * (batch, n_head, d, len)K^T -> A第i行 表示第i位的attention分布
        A = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(Q.shape[-1])
        if mask != None: # 由于 Q和K 都可能有部分内容是 pad,
            A = A.masked_fill(mask.unsqueeze(1), -1e9) # exp(-inf)=0 使之不参与softmax # 由于head, 需要括维以广播
        A = torch.softmax(A, dim=-1) # mask完再softmax省时间
        A = self.dropout(A)

        # (batch, n_head, len, len)A * (batch, n_head, len, d)
        # -> (batch, n_head, len, d) -> (batch, len, n_head, d) -> (batch, len, dim)
        Z = torch.matmul(A, V)
        Z = Z.transpose(1, 2)
        Z = Z.reshape(Z.shape[0], Z.shape[1], -1)

        return Z

# %% [code]
class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_feed):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(d_model, d_feed),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(d_feed, d_model),
        )

    def forward(self, x):
        return self.layers(x)

# %% [code]
class ResidualBlk(nn.Module): # Add & Norm
    def __init__(self, shape, blk):
        super().__init__()
        self.norm = nn.LayerNorm(shape)
        self.dropout = nn.Dropout(0.1)
        self.blk = blk

    def forward(self, x, *args):
        return self.dropout(self.norm(x + self.blk(*args)))

# %% [code]
class Encoder(nn.Module):
    class EncoderBlk(nn.Module):
        def __init__(self, d_model, d_feed):
            super().__init__()
            self.self_attn = ResidualBlk(d_model, MultiHeadedSelfAttention(d_model, 8))
            self.feed = ResidualBlk(d_model, PositionwiseFeedForward(d_model, d_feed))

        def forward(self, x, src_self_mask):
            x = self.self_attn(x, *[x, x, x, src_self_mask])
            x = self.feed(x, *[x])
            return x

    def __init__(self, d_model, d_feed, N):
        super().__init__()
        self.blks = clones(self.EncoderBlk(d_model, d_feed), N)

    def forward(self, x, src_self_mask):
        for blk in self.blks:
            x = blk(x, src_self_mask)
        return x

# %% [code]
class Decoder(nn.Module):
    class DecoderBlk(nn.Module):
        def __init__(self, d_model, d_feed):
            super().__init__()
            self.self_attn = ResidualBlk(d_model, MultiHeadedSelfAttention(d_model, 8))
            self.attn = ResidualBlk(d_model, MultiHeadedSelfAttention(d_model, 8))
            self.feed = ResidualBlk(d_model, PositionwiseFeedForward(d_model, d_feed))

        def forward(self, x, trg_self_mask, enc, trg_src_mask):
            x = self.self_attn(x, *[x, x, x, trg_self_mask])
            x = self.attn(x, *[x, enc, enc, trg_src_mask])
            x = self.feed(x, *[x])
            return x

    def __init__(self, d_model, d_feed, N):
        super().__init__()
        self.blks = clones(self.DecoderBlk(d_model, d_feed), N)

    def forward(self, x, trg_self_mask, enc, trg_src_mask):
        for blk in self.blks:
            x = blk(x, trg_self_mask, enc, trg_src_mask)
        return x

# %% [code]
class Transformer(nn.Module):
    def __init__(self, src_V, trg_V, d_model, d_feed):
        super().__init__()
        self.src_emb = Embedding(src_V, d_model)
        self.trg_emb = Embedding(trg_V, d_model)
        self.encoder = Encoder(d_model, d_feed, 6)
        self.decoder = Decoder(d_model, d_feed, 6)
        self.pred = nn.Linear(d_model, trg_V)

    def forward(self, src, trg):
        src_embed = self.src_emb(src)
        src_self_mask = get_attn_pad_mask(src, src) # 让src的query不要去看src里没用的pad
        enc = self.encoder(src_embed, src_self_mask)
        
        trg_embed = self.trg_emb(trg)
        trg_self_mask = get_attn_pad_mask(trg, trg) | get_attn_subsequence_mask(trg)
        # 让trg的query不去看trg里没用的pad, 并且第i个query只能看到前i个key(生成的后续未知性)
        # 这样第i个输出, 就是根据前i个词预测的生成的下一个词
        trg_src_mask = get_attn_pad_mask(trg, src) # 让trg的query不要去看src里没用的pad
        # 这里不用设置未知性, 因为翻译的对应词可以前可以后, 之类的
        dec = self.decoder(trg_embed, trg_self_mask, enc, trg_src_mask)
        
        res = self.pred(dec)
        return res

# %% [markdown]
# ## Load Data

# %% [code]
class MTData:
    def __init__(self):
        ## fix
        src = np.load(root/"input"/"preprocessing-mt-eng-french"/"fr.npy", allow_pickle=True).tolist()
        trg = np.load(root/"input"/"preprocessing-mt-eng-french"/"en.npy", allow_pickle=True).tolist()

        self.src_vocab = ['<pad>'] + list(set.union(*[set(s) for s in src]))
        self.trg_vocab = ['<pad>', '<s>', '</s>'] + list(set.union(*[set(s) for s in trg]))
        
        self.src_V = len(self.src_vocab)
        self.trg_V = len(self.trg_vocab)
        
        src2idx = {w: i for i, w in enumerate(self.src_vocab)}
        trg2idx = {w: i for i, w in enumerate(self.trg_vocab)}
              
        ## dynamic
        prefix = 1000000
        src, trg = src[:prefix], trg[:prefix]
        
        self.src_maxlen = max([len(s) for s in src])
        self.trg_maxlen = max([len(s) for s in trg]) + 1 # for <s> or </s>
        
        ## fix
        self.src_input = [[src2idx[w] for w in s] for s in src]
        self.trg_input = [[1] + [trg2idx[w] for w in s] for s in trg] # <s> + sentence
        self.trg_output = [[trg2idx[w] for w in s] + [2] for s in trg] # sentence + </s>
        
datas = MTData()

# %% [code]
print(f"French sequence's max length: {datas.src_maxlen}, vocabulary size: {datas.trg_V}")
print(f"English sequence's max length: {datas.trg_maxlen}, vocabulary size: {datas.src_V}")

# %% [markdown]
# ## Dataset

# %% [code]
class MT_Dataset(Data.Dataset):
    sets = train_test_split(datas.src_input, datas.trg_input, datas.trg_output, test_size=0.2)
    
    def __init__(self, kind):
        self.src_input, self.trg_input, self.trg_output = self.sets[::2] if kind=='train' else self.sets[1::2]

    def __len__(self):
        return len(self.src_input)
    
    def __getitem__(self, idx):
        return self.src_input[idx], self.trg_input[idx], self.trg_output[idx]
    
def collate_fn(batch):
    src_maxlen = max([len(item[0]) for item in batch])
    trg_maxlen = max([len(item[1]) for item in batch])
    #print(src_maxlen, trg_maxlen)
    
    a, b, c = (
        pad_sequences(maxlen=src_maxlen, sequences=[item[0] for item in batch], padding="post", value=0),
        pad_sequences(maxlen=trg_maxlen, sequences=[item[1] for item in batch], padding="post", value=0),
        pad_sequences(maxlen=trg_maxlen, sequences=[item[2] for item in batch], padding="post", value=0),
    )
    
    return apply_cuda(torch.LongTensor(a), torch.LongTensor(b), torch.LongTensor(c))

# %% [markdown]
# ## Train utils

# %% [code]
def filter(s):
    if s=='<s>' or s=='</s>' or s=='<pad>':
        return ''
    else: return s
    
def testoutput(src, trg, p):
    print()
    print("fr:", ' '.join([filter(datas.src_vocab[idx]) for idx in src]))
    print("en:", ' '.join([filter(datas.trg_vocab[idx]) for idx in trg]))
    print("pr:", ' '.join([filter(datas.trg_vocab[idx]) for idx in torch.argmax(torch.softmax(p, dim=1), dim=1)]))
    print()

# %% [markdown]
# ## Validation

# %% [code]
def Validation():
    batch_size = 1
    dataset = MT_Dataset("train")
    loader = Data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    model = apply_cuda(Transformer(datas.src_V, datas.trg_V, d_model=512, d_feed=2048))
    
    #params_path = root/"input"/"transformer-mt-french-eng"/"params.ckpt"
    params_path = root/"working"/"params.ckpt"
    model.load_state_dict(torch.load(params_path))
    
    model = model.eval()
    with torch.no_grad():

        for src_input, trg_input, trg_output in loader:
            p = model(src_input, trg_input)
            testoutput(src_input[0], trg_output[0], p[0])
            
            trg_input = apply_cuda(torch.LongTensor([[1]])) # <s>
            pre_output = []
            i = 0
            while True:
                p = model(src_input, trg_input)
                p = torch.argmax(torch.softmax(p[0][i], dim=0), dim=0)
                if p == 2: break
                trg_input = torch.cat((trg_input, apply_cuda(torch.LongTensor([[p]]))), dim=1)
                pre_output += [p]
                i += 1
            print("pr2:", ' '.join([filter(datas.trg_vocab[idx]) for idx in pre_output]))
            print()
            break

# %% [markdown]
# ## Train

# %% [code]
if True:
    batch_size = 20
    dataset = MT_Dataset("train")
    loader = Data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    model = apply_cuda(Transformer(datas.src_V, datas.trg_V, d_model=512, d_feed=2048))
    
    params_path = root/"input"/"transformer-mt-french-eng"/"params.ckpt"
    #params_path = root/"working"/"params.ckpt"
    output_path = root/"working"/"params.ckpt"
    if params_path.exists() and True:
        print("load state: True")
        model.load_state_dict(torch.load(params_path))
    else:
        print("load state: False")
        for p in model.parameters():
            if p.dim() > 1: nn.init.xavier_uniform_(p)
        
    optim = torch.optim.Adam(model.parameters(), lr=1e-5)
    criter = nn.CrossEntropyLoss(ignore_index=0) # 忽略pad
  
    losy = []
    for epoch in range(30):
        lsum = 0
        tot = 0
        for src_input, trg_input, trg_output in loader:
            p = model(src_input, trg_input)
            loss = criter(p.transpose(1, 2), trg_output)
            lsum += loss.item()
            optim.zero_grad()
            loss.backward()
            optim.step()
            tot += batch_size
            print(f"\repoch{epoch}: {tot} / {len(dataset)}, total lost = {lsum / tot}", end=' ')
            if tot % 10000 == 0:
                testoutput(src_input[0], trg_output[0], p[0])
                torch.save(model.state_dict(), output_path)
                Validation()
        losy.append(lsum)
        plt.plot(list(range(epoch + 1)), losy, marker='x')
        plt.show()
        torch.save(model.state_dict(), output_path)
```

