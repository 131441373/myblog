### 信息
对于计算机传输的二进制: 每获得一位, 就消除了一些不确定性

事件概率越低, 信息量越大: (每天太阳都升起, 没啥信息量) -> 信息量可写为 $I = f(p)$

两个不相关事件同时发生, 信息量为两者叠加, 概率为两者乘积 -> 对数形: $I = \log_2 (\frac 1 p)$

(另一种理解方式: p为2幂时, 平均信息量对应了最优编码的平均编码长度)

### 熵
#### 自信息
平均信息量

$$I(X;X) := H(X) := \sum p(x) \log_2(\frac 1 p)$$

#### 联合熵
对应二元随机变量p(x,y)的, 其信息量为 $\log_2 (\frac 1 {p(x, y)})$

$$H(X, Y) = \sum_x \sum y p(x,y) \log_2(\frac 1 {p(x,y)})$$

#### 条件熵
$H(X | y) = \sum_x p(x|y) \log_2(\frac 1 {p(x|y)})$

定义$H(X | Y)$ 为$y$取$Y$中各值的条件熵的平均值:

$$H(X | Y) = \sum_y p(y) \sum_x p(x|y) \log_2(\frac 1 {p(x|y)}) = \sum_x \sum_y p(x, y) \log_2(\frac 1 {p(x|y)})$$

#### 熵的连锁法则

将 $p(x|y) = \frac {p(x, y)} {p(y)}$ 代入上式
得 

$$H(X,Y) = H(Y) + H(X|Y) = H(X) + H(Y|X)$$

#### 互信息
将上式移项:

$$I(X; Y) := H(X) - H(X|Y) = H(Y) - H(Y|X)$$

这个表示: 在已知Y得条件下,能减少多少X的不确定性, 即Y透漏给X的信息量

$I(X;Y) = I(Y;X)$

#### 可视化

![](https://img2020.cnblogs.com/blog/1086046/202006/1086046-20200628193821766-1188041595.png)
把圆理解为: 信息量(不确定性)的范围
H(X, Y)为总的不确定性
I(X;Y)为不确定性的交集
H(Y|X)表示X已知时, Y还剩多少不确定性, 所以相当于 在Y集合中 挖去 X, 也即挖掉他们共有的信息 $I(X;Y)$

#### 相对熵(又叫做 KL散度)
当预测的概率分布q与实际的概率分布p不同时, 实际跑起来(按p)的编码长度期望值会有不同
为

$$D(p||q) = \sum_x [p(x) \log_2 \frac 1 {q(x)} - p(x) \log_2 \frac 1 {p(x)}] = p(x) \log_2 \frac {p(x)} {q(x)}$$

可以证明**互信息为 按照 X,Y独立而估计的概率分布 与 实际概率分布p(x,y)的熵差**

$$I(X,Y) = D(p(x,y) || p(x)p(y))$$

proof. 
将 $H(X|Y)$中$\log_2(\frac 1 {p(x|y)})$ 换为 $\log_2 (\frac {p(y)}{p(x,y)})$, 代入 $I(X;Y)$中即可

#### 交叉熵
由相对熵的定义
当预测的概率分布为q时, 则

$$H(X, q) = H(X) + D(p || q) = \sum_x p(x) \log_2 \frac 1 {q(x)}$$

从这个形式上看, $\log_2 \frac 1 {q(x)}$就是信息量
这可以理解为: 按照概率分布q确定的编码方式在处理实际情况时的平均bit数