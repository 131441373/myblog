## Seq2Seq
Encoder:
Embedding + BiRNN(LSTM/GRU) 保留全部信息(类似NER识别)

Decoder: 
做一个序列生成, 最初输入为 `<s>`
RNN上一个Cell的隐藏层结果记为 $s_{i-1}$

用 $s_{i-1}^Ｔ W h_j$ 依次算出encoder各个结果对当前翻译到的地方的相关度 (attention_weight)
将encoder的各个结果按照weight加权平均 (weight加个softmax来normalize)

将RNN上一个Cell的输出 $y_{i-1}$ embedding后与上述加权后的encoder结果cat
用于跑第 $i$ 个Cell
(训练时, 可以随机的提供teacher_mode, 即上一个cell的预测结果修正(但是loss还是照常算)

## Transformer
[paper](https://arxiv.org/pdf/1706.03762.pdf)
[tutor](https://www.bilibili.com/video/BV1J441137V6?from=search&seid=18415875457697855524)

![结构图](https://img2020.cnblogs.com/blog/1086046/202008/1086046-20200813143752282-872600848.png)

N个结构同, 参数不共享
### self-attention
词向量乘三个不同的矩阵, 得到 q, k, v 三个词向量 (query, key, value)

对于序列中的**每一个**元素, 依次做:

1.将其q 与 序列所有的k去做attention
score用dot product就好(q,k维度同记为 $d$),  然后除一个$\sqrt d$(todo) , 然后再softmax

2.利用attention_weight对 序列所有的v进行加权平均

3.得到的结果作为 该元素的 新向量

善用矩阵, 上述表达都能用矩阵简洁表示, 这样gpu好优化

#### Multi-head self-attention
q乘k个不同的矩阵, 得到  q1...qk
k, v 同理生成k个head

每种head内部同self-attention那样做得到 新向量
不同head得到的向量cat起来, 作为结果

(self-attention类比CNN, multi-head类比多个channel的filter)

#### Masked attention
生成时用的, 因为生成的时候, 只有部分序列, 后面的都还没生成, 算attention没用, 用mask筛掉

#### 可视化 + 理解
经过论文的可视化
self-attention可以识别一些phrase, 代词指代关系, 等等信息

### Position Encoding
[这个解释sincos方法解释的挺好的](https://zhuanlan.zhihu.com/p/121126531)
[上面那个的英文版](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)

attention机制视角太广, 可能看到一些没用的信息 (所以seq2seq后来有local attention方法(虽然效果一般))

transformer中, 对wordEmbedding的结果 "添加" 位置信息
具体是 给序列中第pos个嵌入向量 加一个超参数向量 $e_{pos}$ (论文没有使用拼接)

$$e_{pos}[2i+j] = f_j \left(\frac {pos} {10000^{2i / embed-dim}} \right)$$

其中 $j \in [0, 1]$
其中 $f_j$ 在 $j=0$ 时 为 $sin$, 否则为 $cos$

### Add & Norm
Add就是残差网络的处理方式 (将输入 往 输出 加)
加完之后 使用 LayerNorm

### FeedForward
Linear维度放大 + ReLU + Linear维度还原
用于提升信息表达能力

### Encoder
输入embed+position encoding的vec

跑 Add & Norm 的 Multihead
然后 Add& Norm 的 FeedForward

跑 N=6 次

### Decoder
输入embed+position encoding的vec

跑 Add & Norm 的 Multihead
然后用上一个的输出当 Q, encoder的结果当 K, V, 跑一个Multihead
然后 Add& Norm 的 FeedForward

跑 N=6次

### Mask
encoder: self_attention时: Q不希望匹配到pad的K, 所以矩乘后, 将pad处的K设成 -1e9, 这样softmax的时候贡献就为0了
decoder: self_attention时: 同理上, 由于是生成模型, 额外加一个上三角mask, 使得第i个Q只能匹配前i个K, 这样符合生成的未知性, 最后结果第i个就是前i个序列的预测
decoder: attention时: 同理1, 把encoder输入中的pad处K设成 -1e9

### 训练的循序渐进
先拿长度 <10的句子训练, 1e-1的lr激进点, 让他大致能预测出几个单词就好 (很可能经常预测高频词 I you you)
然后稍微加长一丢丢, 1e-2的练, 慢慢他就不只是预测高频词了, 模型稍稍变好)
再稍微加长一丢丢, 1e-3的练 (这时已经能较好的利用前面热身的效果了, 翻译效果初现)
差不多就丢全数据把, 1e-4的练 

## Seq2Seq模板(不知道行不行的)
```
class Encoder(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.emb = nn.Embedding(input_dim, hidden_dim)
        self.rnn = nn.GRU(hidden_dim, hidden_dim//2, batch_first=True, bidirectional=True)
    
    def forward(self, x):
        x = self.emb(x)
        x, _ = self.rnn(x)
        return x
    
class DecoderCell(nn.Module):
    def __init__(self, hidden_dim, output_dim):
        super().__init__()
        self.emb = nn.Embedding(output_dim, hidden_dim)
        self.rnn = nn.GRUCell(hidden_dim*2, hidden_dim)
        self.out = nn.Linear(hidden_dim, output_dim)
        self.W = nn.Linear(hidden_dim, hidden_dim, bias=False)
        self.softmax = nn.Softmax(dim=-1)
        
    def forward(self, y_pre, h_pre, h_enc):
        
        #(batch, len, dim) matmul (batch, dim, 1) -> (batch, len)
        attn_weight = torch.bmm( h_enc, self.W(h_pre).unsqueeze(-1) ).squeeze(-1)
        attn_weight = self.softmax(attn_weight)
        
        #(batch, 1, len) matmul (batch, len, dim) -> (batch, dim)
        h_attn = torch.bmm( attn_weight.unsqueeze(1), h_enc ).squeeze(1)
        
        y_pre = self.emb(y_pre)
        x = torch.cat((y_pre, h_attn), dim=1)
        h = self.rnn(x)
        y = self.out(h)
        return y, h

class Net(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, st_tag, length):
        super().__init__()
        self.encoder = Encoder(input_dim, hidden_dim)
        self.decoderCell = DecoderCell(hidden_dim, output_dim)
        self.hidden_dim = hidden_dim
        self.st_tag = st_tag
        self.length = length
        
    def forward(self, x, teacher=None):
        h_enc = self.encoder(x) # -> (batch, len, GRU_output*2)

        y_pre = apply_cuda(torch.LongTensor([self.st_tag] * x.shape[0]))
        h_pre = apply_cuda(torch.zeros(x.shape[0], self.hidden_dim))
        
        y = []
        for i in range(self.length):
            y_pre, h_pre = self.decoderCell(y_pre, h_pre, h_enc)
            y += [y_pre]
            y_pre = teacher[:, i] if teacher!=None else torch.argmax(torch.softmax(y_pre, dim=-1), dim=-1)
        
        return torch.stack(y, dim=-1)
```

## Transformer模板

```
# %% [code]
import numpy as np
import pandas as pd

from keras.preprocessing.sequence import pad_sequences

from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
import torch.utils.data as Data
from copy import deepcopy

from pathlib import Path

import matplotlib.pyplot as plt
plt.style.use("seaborn-paper")

# %% [code]
root = Path().resolve().parent

# %% [code]
def apply_cuda(*args):
    if torch.cuda.is_available():
        args = tuple(map(lambda x: x.cuda(), args))
    return args if len(args) > 1 else args[0]

# %% [markdown]
# ## Net

# %% [code]
def clones(x, times):
    return nn.ModuleList([deepcopy(x) for i in range(times)])

# %% [code]
def get_attn_pad_mask(seq_q, seq_k):
    return (seq_k == 0).unsqueeze(1).expand(seq_k.shape[0], seq_q.shape[1], seq_k.shape[1]) # 0 为 pad

def get_attn_subsequence_mask(seq):
    return apply_cuda(torch.triu(torch.ones( (seq.shape[0], seq.shape[1], seq.shape[1]) ), diagonal=1) == 1)

# %% [code]
class Embedding(nn.Module):
    maxlen = 100

    def __init__(self, input_dim, output_dim):
        super().__init__()
        # embedding
        self.emb = nn.Embedding(input_dim, output_dim)
        self.output_dim = output_dim

        # position encoding
        coef = torch.mm(
            torch.arange(0, self.maxlen).unsqueeze(1).float(),
            torch.exp(-1 * torch.arange(0, output_dim, 2) * (np.log(10000.0) / output_dim)).unsqueeze(0)
        )
        self.pe = apply_cuda(torch.zeros(self.maxlen, output_dim))
        self.pe[:, 0::2] = torch.sin(coef) # 偶数
        self.pe[:, 1::2] = torch.cos(coef) # 奇数

        self.norm = nn.LayerNorm(output_dim)
        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        x = self.emb(x) * np.sqrt(self.output_dim) + self.pe[:x.shape[1]] # todo: why mult sqrt dim
        return self.dropout( self.norm(x) ) # +pe自动补全batch维度

# %% [code]
class MultiHeadedSelfAttention(nn.Module):
    class MultiHead(nn.Module):
        def __init__(self, d_model, n_head):
            super().__init__()
            self.W = nn.Linear(d_model, d_model)  # 分n_head个矩阵乘 --优化为-> 乘完再拆
            self.n_head = n_head

        def forward(self, x):  # -> (batch, n_head, len, dim)
            return self.W(x).reshape(x.shape[0], x.shape[1], self.n_head, -1).transpose(1, 2)

    def __init__(self, d_model, n_head):
        super().__init__()
        self.dim = d_model // n_head
        self.n_head = n_head
        self.Ws = clones(self.MultiHead(d_model, n_head), 3)
        self.dropout = nn.Dropout(0.1)

    def forward(self, Q, K, V, mask=None):
        Q, K, V = (W(x) for W, x in zip(self.Ws, (Q, K, V))) # 转为多个head

        # (batch, n_head, len, d)Q * (batch, n_head, d, len)K^T -> A第i行 表示第i位的attention分布
        A = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(Q.shape[-1])
        if mask != None: # 由于 Q和K 都可能有部分内容是 pad,
            A = A.masked_fill(mask.unsqueeze(1), -1e9) # exp(-inf)=0 使之不参与softmax # 由于head, 需要括维以广播
        A = torch.softmax(A, dim=-1) # mask完再softmax省时间
        A = self.dropout(A)

        # (batch, n_head, len, len)A * (batch, n_head, len, d)
        # -> (batch, n_head, len, d) -> (batch, len, n_head, d) -> (batch, len, dim)
        Z = torch.matmul(A, V)
        Z = Z.transpose(1, 2)
        Z = Z.reshape(Z.shape[0], Z.shape[1], -1)

        return Z

# %% [code]
class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_feed):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(d_model, d_feed),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(d_feed, d_model),
        )

    def forward(self, x):
        return self.layers(x)

# %% [code]
class ResidualBlk(nn.Module): # Add & Norm
    def __init__(self, shape, blk):
        super().__init__()
        self.norm = nn.LayerNorm(shape)
        self.dropout = nn.Dropout(0.1)
        self.blk = blk

    def forward(self, x, *args):
        return self.dropout(self.norm(x + self.blk(*args)))

# %% [code]
class Encoder(nn.Module):
    class EncoderBlk(nn.Module):
        def __init__(self, d_model, d_feed):
            super().__init__()
            self.self_attn = ResidualBlk(d_model, MultiHeadedSelfAttention(d_model, 8))
            self.feed = ResidualBlk(d_model, PositionwiseFeedForward(d_model, d_feed))

        def forward(self, x, src_self_mask):
            x = self.self_attn(x, *[x, x, x, src_self_mask])
            x = self.feed(x, *[x])
            return x

    def __init__(self, d_model, d_feed, N):
        super().__init__()
        self.blks = clones(self.EncoderBlk(d_model, d_feed), N)

    def forward(self, x, src_self_mask):
        for blk in self.blks:
            x = blk(x, src_self_mask)
        return x

# %% [code]
class Decoder(nn.Module):
    class DecoderBlk(nn.Module):
        def __init__(self, d_model, d_feed):
            super().__init__()
            self.self_attn = ResidualBlk(d_model, MultiHeadedSelfAttention(d_model, 8))
            self.attn = ResidualBlk(d_model, MultiHeadedSelfAttention(d_model, 8))
            self.feed = ResidualBlk(d_model, PositionwiseFeedForward(d_model, d_feed))

        def forward(self, x, trg_self_mask, enc, trg_src_mask):
            x = self.self_attn(x, *[x, x, x, trg_self_mask])
            x = self.attn(x, *[x, enc, enc, trg_src_mask])
            x = self.feed(x, *[x])
            return x

    def __init__(self, d_model, d_feed, N):
        super().__init__()
        self.blks = clones(self.DecoderBlk(d_model, d_feed), N)

    def forward(self, x, trg_self_mask, enc, trg_src_mask):
        for blk in self.blks:
            x = blk(x, trg_self_mask, enc, trg_src_mask)
        return x

# %% [code]
class Transformer(nn.Module):
    def __init__(self, src_V, trg_V, d_model, d_feed):
        super().__init__()
        self.src_emb = Embedding(src_V, d_model)
        self.trg_emb = Embedding(trg_V, d_model)
        self.encoder = Encoder(d_model, d_feed, 6)
        self.decoder = Decoder(d_model, d_feed, 6)
        self.pred = nn.Linear(d_model, trg_V)

    def forward(self, src, trg):
        src_embed = self.src_emb(src)
        src_self_mask = get_attn_pad_mask(src, src) # 让src的query不要去看src里没用的pad
        enc = self.encoder(src_embed, src_self_mask)
        
        trg_embed = self.trg_emb(trg)
        trg_self_mask = get_attn_pad_mask(trg, trg) | get_attn_subsequence_mask(trg)
        # 让trg的query不去看trg里没用的pad, 并且第i个query只能看到前i个key(生成的后续未知性)
        # 这样第i个输出, 就是根据前i个词预测的生成的下一个词
        trg_src_mask = get_attn_pad_mask(trg, src) # 让trg的query不要去看src里没用的pad
        # 这里不用设置未知性, 因为翻译的对应词可以前可以后, 之类的
        dec = self.decoder(trg_embed, trg_self_mask, enc, trg_src_mask)
        
        res = self.pred(dec)
        return res

# %% [markdown]
# ## Load Data

# %% [code]
class MTData:
    def __init__(self):
        ## fix
        src = np.load(root/"input"/"preprocessing-mt-eng-french"/"fr.npy", allow_pickle=True).tolist()
        trg = np.load(root/"input"/"preprocessing-mt-eng-french"/"en.npy", allow_pickle=True).tolist()

        self.src_vocab = ['<pad>'] + list(set.union(*[set(s) for s in src]))
        self.trg_vocab = ['<pad>', '<s>', '</s>'] + list(set.union(*[set(s) for s in trg]))
        
        self.src_V = len(self.src_vocab)
        self.trg_V = len(self.trg_vocab)
        
        src2idx = {w: i for i, w in enumerate(self.src_vocab)}
        trg2idx = {w: i for i, w in enumerate(self.trg_vocab)}
              
        ## dynamic
        prefix = 1000000
        src, trg = src[:prefix], trg[:prefix]
        
        self.src_maxlen = max([len(s) for s in src])
        self.trg_maxlen = max([len(s) for s in trg]) + 1 # for <s> or </s>
        
        ## fix
        self.src_input = [[src2idx[w] for w in s] for s in src]
        self.trg_input = [[1] + [trg2idx[w] for w in s] for s in trg] # <s> + sentence
        self.trg_output = [[trg2idx[w] for w in s] + [2] for s in trg] # sentence + </s>
        
datas = MTData()

# %% [code]
print(f"French sequence's max length: {datas.src_maxlen}, vocabulary size: {datas.trg_V}")
print(f"English sequence's max length: {datas.trg_maxlen}, vocabulary size: {datas.src_V}")

# %% [markdown]
# ## Dataset

# %% [code]
class MT_Dataset(Data.Dataset):
    sets = train_test_split(datas.src_input, datas.trg_input, datas.trg_output, test_size=0.2)
    
    def __init__(self, kind):
        self.src_input, self.trg_input, self.trg_output = self.sets[::2] if kind=='train' else self.sets[1::2]

    def __len__(self):
        return len(self.src_input)
    
    def __getitem__(self, idx):
        return self.src_input[idx], self.trg_input[idx], self.trg_output[idx]
    
def collate_fn(batch):
    src_maxlen = max([len(item[0]) for item in batch])
    trg_maxlen = max([len(item[1]) for item in batch])
    #print(src_maxlen, trg_maxlen)
    
    a, b, c = (
        pad_sequences(maxlen=src_maxlen, sequences=[item[0] for item in batch], padding="post", value=0),
        pad_sequences(maxlen=trg_maxlen, sequences=[item[1] for item in batch], padding="post", value=0),
        pad_sequences(maxlen=trg_maxlen, sequences=[item[2] for item in batch], padding="post", value=0),
    )
    
    return apply_cuda(torch.LongTensor(a), torch.LongTensor(b), torch.LongTensor(c))

# %% [markdown]
# ## Train utils

# %% [code]
def filter(s):
    if s=='<s>' or s=='</s>' or s=='<pad>':
        return ''
    else: return s
    
def testoutput(src, trg, p):
    print()
    print("fr:", ' '.join([filter(datas.src_vocab[idx]) for idx in src]))
    print("en:", ' '.join([filter(datas.trg_vocab[idx]) for idx in trg]))
    print("pr:", ' '.join([filter(datas.trg_vocab[idx]) for idx in torch.argmax(torch.softmax(p, dim=1), dim=1)]))
    print()

# %% [markdown]
# ## Validation

# %% [code]
def Validation():
    batch_size = 1
    dataset = MT_Dataset("train")
    loader = Data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    model = apply_cuda(Transformer(datas.src_V, datas.trg_V, d_model=512, d_feed=2048))
    
    #params_path = root/"input"/"transformer-mt-french-eng"/"params.ckpt"
    params_path = root/"working"/"params.ckpt"
    model.load_state_dict(torch.load(params_path))
    
    model = model.eval()
    with torch.no_grad():

        for src_input, trg_input, trg_output in loader:
            p = model(src_input, trg_input)
            testoutput(src_input[0], trg_output[0], p[0])
            
            trg_input = apply_cuda(torch.LongTensor([[1]])) # <s>
            pre_output = []
            i = 0
            while True:
                p = model(src_input, trg_input)
                p = torch.argmax(torch.softmax(p[0][i], dim=0), dim=0)
                if p == 2: break
                trg_input = torch.cat((trg_input, apply_cuda(torch.LongTensor([[p]]))), dim=1)
                pre_output += [p]
                i += 1
            print("pr2:", ' '.join([filter(datas.trg_vocab[idx]) for idx in pre_output]))
            print()
            break

# %% [markdown]
# ## Train

# %% [code]
if True:
    batch_size = 20
    dataset = MT_Dataset("train")
    loader = Data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    model = apply_cuda(Transformer(datas.src_V, datas.trg_V, d_model=512, d_feed=2048))
    
    params_path = root/"input"/"transformer-mt-french-eng"/"params.ckpt"
    #params_path = root/"working"/"params.ckpt"
    output_path = root/"working"/"params.ckpt"
    if params_path.exists() and True:
        print("load state: True")
        model.load_state_dict(torch.load(params_path))
    else:
        print("load state: False")
        for p in model.parameters():
            if p.dim() > 1: nn.init.xavier_uniform_(p)
        
    optim = torch.optim.Adam(model.parameters(), lr=1e-5)
    criter = nn.CrossEntropyLoss(ignore_index=0) # 忽略pad
  
    losy = []
    for epoch in range(30):
        lsum = 0
        tot = 0
        for src_input, trg_input, trg_output in loader:
            p = model(src_input, trg_input)
            loss = criter(p.transpose(1, 2), trg_output)
            lsum += loss.item()
            optim.zero_grad()
            loss.backward()
            optim.step()
            tot += batch_size
            print(f"\repoch{epoch}: {tot} / {len(dataset)}, total lost = {lsum / tot}", end=' ')
            if tot % 10000 == 0:
                testoutput(src_input[0], trg_output[0], p[0])
                torch.save(model.state_dict(), output_path)
                Validation()
        losy.append(lsum)
        plt.plot(list(range(epoch + 1)), losy, marker='x')
        plt.show()
        torch.save(model.state_dict(), output_path)
```