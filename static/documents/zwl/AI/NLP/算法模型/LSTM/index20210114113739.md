
### LSTM(long short term memory)
https://zhuanlan.zhihu.com/p/24018768
![如图](https://img2020.cnblogs.com/blog/1086046/202008/1086046-20200808152657948-1141046757.png)


首先将 $x^t$ 和 $h^{t-1}$ 拼接 (左下方两路拼接)
分别乘上四个不同的系数矩阵 (下面的四个分支 对应四种W,ｂ的 W x + b

第一个sigmoid激活变为 $z^f$ 忘记门
第二个外加sigmoid变为 $z^i$ 信息门
第三个外加tanh变为 $z^r$, 记忆门
第四个外加sigmoid变为 $z^o$ 输出门

三个 x (点乘) 分别对应 忘一些, 记一些, 出一些
而 + (点加) 则是 长短期记忆的结合

### BiLSTM
正着跑一遍n个值, 反着跑一遍n个词

下面用 [ , ] 表示 concat

在词标注项目中, 一般是 对应位置拼接
![词标注模型](https://image.jiqizhixin.com/uploads/editor/37a1ae9e-9e95-44e5-8746-e03085f7e7f8/1540354951193.png)

在文本分类项目中, 则是 两个跑到最尽头的信息拼接
![文本分类模型](https://image.jiqizhixin.com/uploads/editor/df55a9f8-422e-4252-a768-9cf4f49bbb56/1540354954203.png)

### 变体 GRU
直接用 1-忘记门 代替 记忆门
进而衍生出了 GRU
由于 "贫穷限制了计算能力" , GRU比LSTM更好训练 (运算量优势)

比如整体结构, 去掉了c, 因为其实你看lstm的时候也会觉得为什么c还要单独来个output门去生成输出不直接用c呢?
比如zt, lstm中的忘一些和记一些能不能用x和1-x表示而不分开两次呢
比如rt, 把输出门的tanh放到dot之后, 和记住信息的tanh融合(ht处)
![如图](https://pic4.zhimg.com/80/v2-f2716bc289734d8b545926b38a224692_720w.png)