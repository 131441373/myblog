### n元语法
定义$S_n = (x_1, \cdots, x_n)$
约定$w_{2-n}$到$w_0$为<BOS>表示句首, $w_{n+1}$为<EOS>表示句尾 (S表示sentense)

$$p(S_n) \approx \prod_{i=1}^n p(w_i | w_{i-n+1}^{i-1})$$

#### 最大似然估计
对训练集, 定义$c(s)$表示s作为子串在训练集中的出现次数
最大似然估计 将 $p(w_i | w_{i-n+1}^{i-1})$　定义为  $\frac {c(w_{i-n+1}^i)}{c(w_{i-n+1}^{i-1})}$
于是

$$p(S_n) = \prod_{i=1}^n \frac {c(w_{i-n+1}^i)}{c(w_{i-n+1}^{i-1})}$$

对于多个句子组成的文段 $T = (S_1, \cdots, S_m)$, 视为各句(用标点符号分隔开)之间有若干EOS和BOS

$$p(T) = \prod_{i=1}^m p(S_i)$$

### 模型平滑化处理
由于训练集不可能包含所有所有的cases
按照最大似然估计, 如果某个分子$c(w_{i-n+1}^i)$ 为0, 那就整个计算结果都是0了
平滑处理的基本思想是劫富济贫

#### 加d法
在训练集中, 对词汇表中的单词, 每个放$\delta$个到训练集中
$\delta\in [0, 1]$

$$p(S_n) = \prod_{i=1}^n \frac {\delta + c(w_{i-n+1}^i)}{\delta \times|Vocabulary| + c(w_{i-n+1}^{i-1})}$$

#### Good-Turing法
对于n元语法

$$\frac {c(w_{i-n+1}^i)}{c(w_{i-n+1}^{i-1})} = \frac  {c(w_{i-n+1}^i)}{\sum_x c(w_{i-n+1}^{i-1} x)}$$

于是c统计的都是长度为n的串

将 $c(s)=r$ 的串, 定义 $c^*(s) = r^* = (r+1)\frac {n_r+1}{n_r}$ 

这里 $n_r$ 表示 $c(s)=r$ 的s有多少种

由于 $n_r=0$ 会直接失效, 这个算法只是提供思路

#### Katz法 (又称back-off)


### 语言评判
#### 交叉熵
由于一个词的概率不能用来衡量一个语言模型 
定义 

$$H(S, q) = \lim_{n\to \inf} \frac 1 n \sum_{L_n} p(x_1^n) \log_2 \frac 1 {q(x_1^n)}$$

除n是为了平均化, 归一化标准, 否则句子越长越混乱了

由于某些理论(todo不会), **当n足够大时**

$$H(S, q) \approx \frac 1 n \sum_{L_n} \log_2 \frac 1 {q(x_1^n)}$$

#### 文段困惑度
对于文段$T$, 定义其平均熵为:

$$H_q(T) = \frac 1 {word_{count}} \log_2 \frac 1 {q(T)}$$

困惑度为2的上式次方

