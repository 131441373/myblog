控制参数的复杂度

## 输入标准化
使得数据scale跨度没那么大, optimizer路径更平滑, 等等
对于图片, 见CV文件夹中的pytorch Normalize
对于

## L1正则化
$L + \lambda \sum |\theta|$
需要自己写
```
for x in net.parameters():
    lost += torch.sum(torch.abs(x))
```

## L2正则化weight_decay=
$L + \lambda \sum \theta^2$
在定义optim时添加参数 `weight_decay=0.01` 表示$\lambda$

## 弹性正则化
上面两种结合
$\lambda_1 R_1 + \lambda_2 R_2$

## Dropout
不能过度依赖某个特征
(比如那个特征没了, 或者说你的输出就是只和那个特征有关, 不管别的了
对于一个越多参数的层,  倾向于越大的概率dropout
(如果要用dropout, 可以一开始都不drop(设置概率), 然后到后期overfit了再开

## Norm层 (用于网络之后, 激活层之前)
使得中间层更加稳定, 不太受输入影响导致大规模变化
[tutor](https://www.youtube.com/watch?v=nUUqwaxLnWs&list=PL9fbVgKf1HGxbrfvk4-mqRD_GWn_wOR0q&index=6)
[paper](https://arxiv.org/abs/1502.03167)

### BatchNorm在训练时
作用于中间层, 对于 $batch_i$ 有 tensor $x_i$
同理于一般的norm, 先求平均值 $\mu$, 然后求方差 $\sigma^2$, 然后值转为 $x' = \frac {x-\mu}{\sqrt{\sigma^2 + \epsilon}}$
在batchnorm中, 它还做了个线性激活 $(\gamma x' + \beta)$, 其中 $\gamma,\beta$ 均是可学习的

(同时由于norm的时候引入了一些同一批次的其他data的信息, 这些noise一定程度上也起到一些regularization的作用(类似dropout)
(**但这不是本意, 不要把他当regularization用**)

不含channel型, 只有1d (batch, L)
对于L的每个节点, 都进行batchnorm(L), 例如标准化完, 对于任一节点, 其权值在所有batch加起来和为0

含channel型: 1d对应 (batch, channel, L), 2d对应 (batch, channel, H, W) ...
对于每个channel, 将 (batch,H,W)的 H*W 按batch加权 标准化 `nn.BatchNorm2d(channels)`, 主要用于图像处理
**特别注意!!! 不是整个H*W作为张量去normalize(这样方差没法算的), 而是作为单个元素, 例如标准化完, 对于任一channel, 其batch*H*W所有元素加起来为0**

### BatchNorm在测试时
会根据训练时的 $\mu, \sigma^2$ 按照某个算法弄出测试时的 $\mu$ 和 $\sigma^2$
这个todo

### LayerNorm
[paper](https://arxiv.org/abs/1607.06450)

指定一个shape, 将后几维(shape形)标准化, 前面的维(类似batch)独立
`nn.LayerNorm(tuple)` 接受 (*, tuple) shape 的输入 (*为前面独立的各维的shape)
主要用于语言处理

原理todo

### GroupNorm

论文给出GN对batchsize一点也不敏感, 其norm的效果非常稳定的好
论文给出将channels分成32个group效果较优
`nn.GroupNorm(num_groups, num_channels)`

## Label smoothing
将one-hot标记的one减去一个eps(=0.1)劫富
然后平均分给其他0(济贫)
避免cross entropy的过拟合问题和alleviate一些数据误标问题

有一些其他的smooth方式见书todo后补