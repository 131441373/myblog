## tensor
tensor 类似 numpy.array

#### 创建
Tensor更常用

`torch.Tensor(5, 3)` 创建一个5*3的矩阵， **这样就不要用tensor**， 更具体可以用IntTensor, FloatTensor等指定具体类型
`torch.tensor([1,2,3])` 这样的话**注意不是Tensor, Tensor是FloatTensor的别名**

**注意, nn跑的时候要求float32 而不是 float64(double), 所以Tensor是最合适的**

#### 随机
`torch.rand(1, 2)` 1*2随机矩阵
`torch.randn(1, 2)` 正态分布随机的1*2矩阵
`torch.normal(torch.one(100, 1)*2, 3)` 每项都是 均值为2， 方差为3的随机数生成

打乱数据的话
`perm = torch.randperm(len(dataset))` 生成一个dataset大小的随机排列
`dataset = dataset[perm]` 使用py的索引技巧

#### 操作
`x = x.reshape((2, 5))` 可以将tuple中至多一维设称-1让他自己算
`x = x.transpose(x, y)` 将某两维交换
`x = x.permute(0, 2, 3, 1)` 多维数交换
`x = torch.cat(listofTensors, dim=)` 已有维度上拼接
`x = torch.stack(listofTensors, dim=0)` 在最外层扩张一个维度, list依次放在里面 (可以理解为把list那层换成Tensor的[]
`x = torch.unsqueeze(x, dim=0)` 等价于list中的 `x = [x]`
`y = x.clone()`
`val, idx = x.sort(dim=)` torch的sort还返回index

#### 索引


#### 与numpy互动
`a = numpy.array([1,2])`
`x = torch.tensor(a)`
`a = torch.numpy()`

Tensor与numpy基本兼容, 除非含grad
如果含grad, 需要 `.detach().numpy()`转换一下

#### 与numpy不同
rand相关不用套一层random

mm : matmul， tensor的dot为对应位相乘后相加，**非常符合逻辑！！！！！！！！！！！！！！**

tensor的size()相当于numpy的shape(), tensor.shape没有括号，就是size() 

拷贝用的是.clone

合并连接用的是 cat

#### 与DataFrame互动
`torch.from_numpy(pd_data.values)`

## Variable[Deprecated] -> tensor 
现在功能集到tensor上了
用户创建的requires_grad的tensor含有关于变量的梯度grad
根据函数创建的节点，含grad_fn

```python
x = torch.tensor([3.,2.], requires_grad=True) # x0 = 3, x1 = 2
y = 2 * x[0]**2 + x[1] ## y = 2 x0^2 + x1
print(x) # tensor([3., 2.], requires_grad=True)
print(y) # tensor(20., grad_fn=<AddBackward0>)
y.backward() # y'x0 = 4 x0 = 12, y'x1 = 1
print(x.grad) # tensor([12.,  1.])
```

## net的parameter()
net.parameter返回一个迭代器
其中每个元素依次是net中一个连接层的参数表
结构为:
对于n*m全连接层, 为一个m*n的参数表(**注意反的**)
对于激活函数(n->n), 为一个n的一维参数表 

## 常用板子
`import torch.nn as nn`

#### 常规架构
```python
net = nn.Sequential(
    nn.Linear(2, 10), # 全连接层， 得到的结果不激活
    nn.Linear(10, 1), nn.Sigmoid() # 全连接层， 得到的结果用Sigmoid激活 （激活也可以看作是一个只有横向对应连接的层
    #依次类推
)
```

#### 自定义架构
```python
class Net(nn.Module):
    def __init__(self, ninput, nmid, noutput):
        super(Net, self).__init__() # 要求如此
        self.l1 = torch.nn.Linear(ninput, nmid) # 全连接层
        self.l2 = torch.nn.Linear(nmid, noutput) # 根据需要建立各层

    def forward(self, x): ## 网络如何连接（FP过程如何实现
        x = self.l1(x)
        x = torch.relu(x) # 每跑完一层，看需要是否增加一个激活函数
        x = self.l2(x)
        return x


net = Net(1, 10, 1) # 构造实例
```

#### 训练
```
optimizer = torch.optim.SGD(net.parameters(), lr=0.5) # 随机梯度下降法， lr = learning rate
lossfunc = torch.nn.MSELoss() # min square err 的 lossfunc

plt.ion() # 需要可视化的话

for t in range(100):
    prediction = net(x)  # FP （x为一组数据而非一个， tensor中一行一个，运行结果也是一组结果
    loss = lossfunc(prediction, y) # 套上loss
    optimizer.zero_grad() # 清空梯度！！！ ！！！ 可以这么理解，符合函数求导的时候，程序通过 += 实现， 所以最开始需要全设为0
    loss.backward() # 反向传播
    optimizer.step() # 根据导数对网络中参数进行优化 # 梯度存在parameter.grad里，step根据这些信息调整

    if t%5==0: # 需要可视化的话
        plt.cla()
        plt.scatter(x, y)
        plt.plot(x, prediction.data.numpy(), c='r')
        plt.pause(0.1)

plt.ioff()
plt.show()
```

#### 分批训练

`import torch.utils.data as Data`

```python
loader = Data.DataLoader(
    dataset=Data.TensorDataset(xs, ys),
    batch_size=10, #一批几个
    shuffle=True # 每批(每次开启loader迭代器)都要先经过shuffle打乱idx表, 因此往Dataset中getitem函数传入的idx参数就是乱的 
)
for epoch in range(20): # 训练轮数
    for x,y in loader: # 分批取出数据
        pass
```

#### 求正确率
以多分类为例子
```
prediction = torch.argmax(model(valX), 1)
accuracy = (prediction == valY).sum().float() / valY.shape[0]
```

#### 自定义dataset


#### 数据预处理
`from torchvision import transform`

#### 预测
https://stackoverflow.com/questions/55627780/evaluating-pytorch-models-with-torch-no-grad-vs-model-eval
```
model = model.eval()
with torch.no_grad():
  pass
```

## 保存
```
# Save and load the entire model.
torch.save(net, 'model.ckpt') # 连通网络结构也保存了
net = torch.load('model.ckpt') ## ckpt - check point

# Save and load only the model parameters (recommended).
torch.save(net.state_dict(), 'params.ckpt')
net = nn.Sequential(....) # 因为只保留了参数，网络需要重新构建
net.load_state_dict(torch.load('params.ckpt'))  # 给重新构建的网络塞进参数
```