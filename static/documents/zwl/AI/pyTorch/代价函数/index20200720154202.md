$L(\Theta)=\frac 1 n \sum_{i=1}^n L(\hat{y_i}, y_i)+ \lambda R(\Theta)$

### L
#### L1Loss 和 MSEloss
逐项求差的绝对值
MSE其实就是L2loss的话还要再平方一下

#### 二分类或多标签二分类交叉熵BCEWithLogitsLoss 和 多分类交叉熵CrossEntropyLoss

**上述两个算法, 均把前置的sigmoid/softmax预处理含在里面了, 不用额外写**

二分类中
先通过sigmoid转到0~1范围
$\hat{y}$ 视为 $p(y=1|x)$
$p(y=0|x) = 1 - p(y=1|x)$
然后处理同下

多分类中
输出先通过softmax转为概率
然后就有 $\hat{y_{[i]}} = p(y=i|x)$
弱正确标签为t
那么要猜对即使 $\hat{y_{[t]}}$ 尽量大
也即 $-\log(\hat{y_[t]})$　尽量小

**代码中正确标签不用表示成one-hot, 标量即可**

#### 等级损失MarginRankingLoss
$L = max(0, margin - y*(f(x) - f(x')))$
其中f是估分器, y=1为希望x比x'分高, y=-1为希望x'比x分高

#### 二分类Hinge:SoftMarginLoss  和 多分类Hinge: MultiMarginLoss
二分类Hinge(SVM损失函数)
$L = max(0, margin-y*\hat{y})$
需要自己写, `torch.mean(torch.clamp(margin - y * prediction, min=0))` (其中clamp是把tensor中每个元素限制在min-max范围, max没指定表示不限制

二分类Hinge的log变种, 可以用SoftMarginLoss
$L= max(0, margin+exp(-y*\hat{y}))$

多分类Hinge
记 $t$ 为正确类别
$k$ 为$\hat{y}$ 中非 $t$ 下标的最大值 ($k=\argmax_{k\neq t} \hat{y_{[k]}}$)
试图使得正确分类t比其他(比其他最大的那个)至少高出margin的间隔
$L=\max(0, margin-(\hat{y_{[t]}}-\hat{y_{[k]}}))$

多分类Hinge的log变种
`torch.mean(torch.clamp(margin + exp(-(prediction[t] - predition[k])), min=0))`\

### 正则化
控制参数的复杂度

#### L1正则化
$L + \lambda \sum |\theta|$
需要自己写
```
for x in net.parameters():
    lost += torch.sum(torch.abs(x))
```

#### L2正则化weight_decay=
$L + \lambda \sum \theta^2$
在定义optim时添加参数 `weight_decay=0.01` 表示$\lambda$

#### 弹性正则化
上面两种结合
$\lambda_1 R_1 + \lambda_2 R_2$

#### dropout丢弃法
训练时令部分神经元失效
net中在连接层和激活层之间添加一个Dropout(p)层

dropout方式也可以理解为训练了一个集成模型