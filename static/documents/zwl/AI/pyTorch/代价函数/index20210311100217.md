$L(\Theta)=\frac 1 n \sum_{i=1}^n L(\hat{y_i}, y_i)+ \lambda R(\Theta)$

### L

**调用统一都是 (prediction, groundtruth)**

#### L1Loss 和 MSEloss
逐项求差的绝对值
MSE其实就是L2loss的话还要再平方一下

#### 二分类或多标签二分类交叉熵BCEWithLogitsLoss 和 多分类交叉熵CrossEntropyLoss

**上述两个算法, 均把前置的sigmoid/softmax预处理含在里面了, 不用额外写**

二分类中
先通过sigmoid转到0~1范围
$\hat{y}$ 视为 $p(y=1|x)$
$p(y=0|x) = 1 - p(y=1|x)$
然后处理同下

多分类中
输出先通过softmax转为概率
然后就有 $\hat{y_{[i]}} = p(y=i|x)$
若正确标签为t
那么要猜对即使 $\hat{y_{[t]}}$ 尽量大
也即 $-\log(\hat{y_[t]})$　尽量小

**代码中正确标签不用表示成one-hot, 标量即可**

**交叉熵p=q时最小, 所以可以用于逼近分布**

##### 高维度
比如序列问题, 一个batch中对应多个分类问题这种
需要将 (batch, d, C) 调整为 (batch, C, d)  (`.transpose(1, 2)`
这样是有理由的
因为torch在进行 softmax操作时, 后者可以更好的一起操作 (设置dim=1)

#### 等级损失MarginRankingLoss
$L = max(0, margin - y*(f(x) - f(x')))$
其中f是估分器, y=1为希望x比x'分高, y=-1为希望x'比x分高

#### 二分类Hinge:SoftMarginLoss  和 多分类Hinge: MultiMarginLoss
二分类Hinge(SVM损失函数)
先让output layer 就1元素
$L = max(0, margin-y*\hat{y})$
需要自己写, `torch.mean(torch.clamp(margin - y * prediction, min=0))` (其中clamp是把tensor中每个元素限制在min-max范围, max没指定表示不限制

二分类Hinge的log变种, 可以用SoftMarginLoss (其实不太一样, 自己写吧)
$L= \log(1+exp(margin-y*\hat{y}))$

多分类Hinge
记 $t$ 为正确类别
$k$ 为$\hat{y}$ 中非 $t$ 下标的最大值 ($k=\argmax_{k\neq t} \hat{y_{[k]}}$)
试图使得正确分类t比其他(比其他最大的那个)至少高出margin的间隔
$L=\max(0, margin-(\hat{y_{[t]}}-\hat{y_{[k]}}))$

多分类Hinge的log变种
`torch.mean(torch.clamp(margin + exp(-(prediction[t] - predition[k])), min=0))`\

### 自己实现loss
#### 基于分布的crossentropy
```
class MultiCrossEntropyLoss(nn.Module):
    def __init__(self):
        super().__init__()
        self.T = 2 # temperature
        self.Q = nn.LogSoftmax(dim=1)
        self.P = nn.Softmax(dim=1)
    def forward(self, pred, gold):
        p = P(gold/self.T)
        logq = Q(pred/self.T)
        return torch.mean(torch.sum(-p * logq, dim=1), dim=0)
```