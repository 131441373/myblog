## 一个网络不同层不同lr
```
optim.SGD([
  {'params': model.base.parameters()}, # 不写则使用最后面定义的值作为默认值
  {'params': model.classifier.parameters(), 'lr': 1e-3} # 写明了, 则用这里写的这个
], lr=1e-2, momentum=0.9)
```

## lr_scheduler
learning rate decay优势: 更容易收敛, 而不是在收敛处附近到处抖动

scheduler.step需要在optimizer后面用, 如
```
scheduler = ...
for epoch in range(100):
  train(...)
  validate(...)
  scheduler.step()
```

### torch.optim.lr_scheduler.xxx
#### LambdaLR 自定义
将lr设为 初始lr 乘上  lr_lambda(epoch)
`scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: np.sin(epoch) / epoch)`
#### MultiplicativeLR 累乘
将lr *= lr_lambda(epoch)
格式同上

#### StepLR 每若干epoch衰减
每step_size个epoch, 将lr *= gamma
`scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)`

#### ExponentialLR = StepLR的step_size=1情形
`lr_scheduler.ExponentialLR(optimizer, gamma)`

#### MultiStepLR epoch次数达里程碑后衰减
如下例, epoch到30时, 到80时分别衰减一次 
`scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)`

#### ReduceLROnPlateau
`(optimizer,mode='min',factor=0.1,patience=10,verbose=False,threshold=0.0001,threshold_mode='rel',cooldown=0,min_lr=0,eps=1e-08)`
mode='min' 表示 监控量停止下降时 decay, 如果修改为 'max' 表示监控量停止上升时 decay
patience 监控时允许一些波动, 例如监控量停止下降需要连续超过patience次才decay
factor 表示衰减时 lr *= factor
threshold 例如监控监控量下降, 如果下降的量误差仅仅在threshold之内, 不算做下降
threshold_mode rel或abs 表示threshold是相对误差还是绝对误差
cooldown lr进行decay之后, 需要先缓冲cooldown轮epoch, 然后再监控
min_lr 设置lr_decay最多跌到哪
eps Minimal decay applied to lr. If the difference between new and old lr is smaller than eps, the update is ignored.

## optimizer策略
Adam优

### momentum
$w -= lr * V_w$
$V_w =  \beta V_w + (1 - \beta) dw$ , 其中隐含了 $beta$被的上一次速度, $beta^2$的上上次速度..等等, 可以理解为有一定的惯性
beta一般默认0.9 (pytorch也如此)

### RMSProp (Root mean square prop)
$w -= lr *  \frac {dw}{\sqrt{S_w}+\epsilon}$
$S_w =  \beta S_w + (1 - \beta) (dw)^2$  
beta一般默认0.999 (pytorch也如此)

### Adam
$\beta_1$ 给 momentum 求 $V_w$
$\beta_2$ 给 RMSProp 求 $S_w$

计算时 分别令 $V'和S'$ 为 $V和W$ 针对对应 $\beta$ 进行 $\frac {1}{1-\beta^t}$ 的衰减, 其中 $t$ 为iter次数
最后 $w -= lr * \frac {V'} {\sqrt{S'} + \epsilon}$