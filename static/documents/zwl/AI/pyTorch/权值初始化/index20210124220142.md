https://www.cnblogs.com/jfdwd/p/11269622.html
https://blog.csdn.net/ys1305/article/details/94332007

## 常见模型
常见模型都有自己的default initialize策略的, 如果不懂, 可以直接用默认的.
比如, linear可以参考pytorch文档里, 搜linear, 进去就可以看到它的weight和bias的初始化方法

## nn.init.xx
法1:
`nn.init.normal_(layer.weight)`
法2:
`layer.weight.data.normal_()`

统一法:
```
def weight_init(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_normal_(m.weight)
        nn.init.constant_(m.bias, 0)
    # 也可以判断是否为conv2d，使用相应的初始化方式 
    elif isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
     # 是否为批归一化层
    elif isinstance(m, nn.BatchNorm2d):
        nn.init.constant_(m.weight, 1)
        nn.init.constant_(m.bias, 0)
```
外面model定义完后 `model.apply(weight_init)`

### normal_
`(tensor, mean=0, std=1)` N(mean, std^2)
### uniform_
`(tensor, a=0, b=1)` U(a, b)
### constant_
`(tensor, value)`
### ones_, zeros_, eye_
`(tensor)`
### xavier_normal_, xavier_uniform_
`(tesnor, gain=1)`
`gain=nn.init.calculate_gain('relu')` 其中relu可改为linear, conv2d, tanh, sigmoid, leaky_relu等

> for a model like conv -> relu -> conv -> relu, you could use: `nn.init.xavier_uniform_(m, gain=nn.init.calculate_gain('relu'))`

> xavier在 tanh 表现好

### kaiming_normal_, kaiming_uniform_ (又称He initialization) (何凯明)
`(tensor, a=0, mode='fan_in', nonlinearity='relu')`
如果是leaky_relu, a要设成leaky的斜率

>Choosing 'fan_in' preserves the magnitude of the variance of the weights in the forward pass. 
Choosing 'fan_out' preserves the magnitudes in the backwards pass.

>论文中表示正常情况下, in,out效果差不多

> if you suspect your backward pass might be more "chaotic" (greater variance) it is worth changing the mode to fan_out. 
This might happen when the loss oscillates a lot 
(e.g. very easy examples followed by very hard ones).

> kaiming在 relu 表现好
