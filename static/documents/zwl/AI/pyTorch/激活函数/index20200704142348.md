
如果没有激活函数

那么（x*边系数+常数）*边系数+常数... 最后永远也还是只有一个x，仍然是线性的

所以需要掰弯，但又不能太复杂， 不然求导数麻烦

#### ReLU

$$ReLu(x) := max(x, 0)$$

比较好，一般用这个

导数非0即1，但是在0那会导致梯度消失，称为死区，为此产生了更进一步的leakyReLU, Maxout

仅用于hidden layer，不用于输出层（否则一半的数据莫得了）

#### Sigmoid / Tanh

$$ Sigmoid(x):=\frac 1 {1 + e^{-x}}$$

$$Tanh(x) := \frac {e^x - e^{-x}}{e^x + e^{-x}} = \frac {e^{2x} - 1}{e^{2x}+1}$$

Sigmoid函数的导数，两边平，中间凸起，最大值只有1/4,  而且凸起的左右范围很窄，即非常容易取很小的值，BP时一路积累可能梯度消失

Tan函数导数形状类似，最大值有1，但是凸起的左右范围仍然比较窄，同样容易出现梯度消失

存在梯度消失问题，不建议在hidden layer使用 [reason](https://blog.csdn.net/qq_37667364/article/details/88806870)

#### Softmax

softmax(a,b)给出根据a，b的大小相差程度，给出概率，让大概率a，小概率b

具体的，softmax给出与输入维度相同的向量值函数，表示各值的概率

$$softmax(v_1, v_2,... , v_n) = (\frac {e^{v_1}} {\sum e^{v_i}}, \frac {e^{v_2}} {\sum e^{v_i}}, \dots)$$