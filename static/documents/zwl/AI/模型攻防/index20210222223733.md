
# 白箱攻
知道network的参数和架构
## constraint
$d(x', x_0) \le \epsilon$
d的可能的定义方法: (图片看上去后者好一些
1) $L_2$ **(图像上看是个圆/球)**
2) $L_{\infty}$: 每个值得delta的绝对值的最大值 **(图像上看是个方型/立方体)**

## target
$y'$ 和 $y_0$ 尽量远 (若还有攻击目标, 则还要 $y'$ 和 $y_{target}$ 尽量近

使用gradient descent
初始点为 $x_0$
每次step之后, 做一个fix, 如果跳出了constraint区域, 则拉回来限制边界上离目标最近的地方
(例如圆就圆心连一下, 方就几个if

### FGSM
$x' \leftarrow x_0 - \epsilon \Delta x$

**只step一次**, $\Delta x$ 定义为gradient每个元素作用sign函数之后的结果, $\epsilon$ 就是constraint里那个

这样的方法本质上是使用了 $L_{\infty}$, 然后一步走到立方体的某个顶角

### 其他
Basic iterative method (https://arxiv.org/abs/1607.02533)
L-BFGS (https://arxiv.org/abs/1312.6199)
Deepfool (https://arxiv.org/abs/1511.04599)
JSMA (https://arxiv.org/abs/1511.07528)
C&W (https://arxiv.org/abs/1608.04644)
Elastic net attack (https://arxiv.org/abs/1709.04114)
Spatially Transformed (https://arxiv.org/abs/1801.02612)
One Pixel Attack (https://arxiv.org/abs/1710.08864)

# 黑箱攻
不知道network的参数和架构

如果知道训练数据, 那就找个架构用知道的训练数据去train, 然后有几率就成功

# 防
防御方式不能泄露

## passive defense
例如使用anomally detection判断图片是否被恶意修改

图片进入模型之前加一个filter, 例如smoothing / resize+padding
可以加几种不同的filter, 分别和不加filter的模型比对, 如果相差比较大(大于某个阈值)就是被攻击了

## proactive defense
**"自己攻击自己"**, 以此找出漏洞, 自己修补
train完model之后
迭代T个epoch
对于trainset的每个x找出attack的$x'$并标注上和原来一样的label
用这些数据来tune一下

缺点: 别人换个攻击方法就行, 所以防御方需要尽可能多的找攻击方法, 攻击方也要尽可能不要让攻击方法泄露