Life Long Learning

一方面避免机器忘记旧的, 另一方面不能固执要去接受新的知识

当前遇到的问题就是: 按顺序去train几个task, 比几个task的数据放一起打乱了train效果差, 而且往往是以前的知识忘得特别多
但是所有数据放一起multi-task去train虽然效果好, 但是训练时间长, 空间需求大, 很难做到

# Elastic Weight Consolidation (EWC)
基本idea: 少动模型中的重要参数, 侧重于改对原模型不重要的那些参数

例如 $L' = L + \lambda \sum b_i (\theta_i - \theta_i^b)^2$ 其中 $\theta_i^b$ 为原模型中的值, $b_i$ 为该参数的重要程度(越大越重要)

计算 $b_i$ 的几种方法:
1) EWC: L对参数求二阶导(所有数据取平均).   如果二阶导比较小, 那么改变它对原模型影响不大, 否则稍微改一下原模型L就变挺多 (盆地底往不同方向平坦程度不同
2) MAS: 模型输出(可以是向量)对于参数的导数(若是向量再L2Norm) (所有数据取平均)

# generate data
用模型产生以前的data, 代替保存以前的data

由于生成仍较难, 尚待研究
[link](https://arxiv.org/pdf/1705.08690.pdf)
[link](https://arxiv.org/pdf/1711.10563.pdf)

# 加new class
如果是分类问题, 可能目标是往类别中增加几个新类, 并提供这几个新类的训练数据 (但是旧类的数据都删啦)

1) LWF: 增加loss: 对于新图, 新模型在旧类别跑出来的distribution和旧模型跑出来的distribution相近(用crossentropy + softmax的1/T次方加强)
2) iCaRL

# 扩model
模型能力已经不够了, 不扩学不下去了

尚待研究
Progressive Neural Network
Expert Gate
Net2Net

# 调一下学习顺序
模拟人类学习, 是有顺序有依赖的

# eval
画一个矩阵 $R_{i,j}$ 表示学完前 $i$ 个task的数据时在第 $j$ 个task的performance
$i<j$ 时显示一种触类旁通能力
$i>j$ 时显示后续遗忘程度/一种学完新知识后发现旧知识有了新理解的能力

$\sum_{i} R_{T, i} - R_{i,i}$ 体现后续遗忘程度/一种学完新知识后发现旧知识有了新理解的能力
$\sum_{i} R_{i-1, i} - R_{0, i}$ 体现触类旁通能力