# network pruning 网络剪枝
模拟人脑, 先往大的发展, 再剪枝

[彩票假设](https://arxiv.org/pdf/1803.03635.pdf)
剪枝后使用大模型的原始初始化权值作为小网络的初始化权值效果好
个人理解: 剪枝是根据大网络剪的, 不同的原始初始化权值训练出来的网络, 最后剪枝完是不同样子的
所以用什么初始权值剪的, 就用什么初始权值初始化小的网络

而在2019年有一篇[文章](rethinking the value of network pruning) argue道: pruning只需要获取新的网络结构就行, 权值不需要沿用, 直接随机初始化更好
这个有待进一步研究[知乎论文原作者回答](https://www.zhihu.com/question/323214798/answer/678706173)好像跟学习率有关系

## prune 权值 还是 神经元
prune权值: 如果是把边删了, 网络非常不对称, 不是矩阵运算, 很难用gpu加速. 所以只能是把prune掉的边权设成0且no_grad. 但是网络并没有变小...所以不好
prune神经元: 比较方便

## 迭代prune
每次prune剩 $p^{1 \over n}$
n次之后剩 $p$ 的参数
一般能到 $p=10\%$

# knowledge distillation 知识蒸馏

train好的大网络fix住, 作为老师, 他除了能一定程度上完成指定任务, 还能有一些其他的知识
例如, mnist分类器, 老师对于一张1的图片给出 1-0.7, 7-0.2, 9-0.1, 学生使用crossentropy去逼近这个概率分布, 能够学到1和7是相像的
一个技巧: softmax的时候将所有值同时除个T这样可以将高的分数和低的分数稍微拉进一点点, 去让学生更好的学到知识

一种应用场景: CV领域有很多类型的网络, 把各类网络的结果ensemble(平均)起来效果能比较好, 但是慢. 让这个ensemble作为老师, 就可以把多个网络整合为一

# parameter quantization 参数

1. 32bit -> 16bit
2. 权重离散化: 将权重聚类, 每个聚类的平均值作为代表值, 并把所有的该聚类的值都设为该平均值, 这样只用记录类别.极致一点甚至可以再加一个huffman编码
grad decend的时候浮点数算, 聚类离散化, 依次类推
3. 权值离散化成 $\pm 1$

# architecture design

1. linear类SVD变化: linear(A, B) 变成 linear(A, mid) + linear(mid, B) 
于是 ${A\times mid + mid\times B \over A\times B} = mid * ({1\over A} + {1\over B})$

2. conv变形: 第一步depthwise conv, 一个channel一个filter, 即每个filter并非三维.但是这样channel之间没有互动.所以第二步是做一个 $1\times 1$ conv
于是 ${k\times k\times inchannel + inchannel \times outchannel \over k\times k\times inchannel \times outchannel} = {1\over O} + {1\over k\times k} \approx  {1\over k\times k} $

# dynamic computation

1. 中间监督, 算力不足就把后面层cut掉. 缺点: CNN的卷积层的前后是有简单, 复杂, 语义的布局的, bert也有词,语法,句的布局. 加了中间监督会影响它的布局
CV领域相关的工作: MSDnet 类似金字塔一般, 除了主干线, 还有短支线,更短支线, cat起来, 这些短支线负责完成中间监督的布局, 长支线负责更深层的监督

