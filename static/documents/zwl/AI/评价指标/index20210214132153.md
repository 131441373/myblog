# 准确率 召回率
positive(P) 和 negative(N) 表示预测结果
true(T) 和 false(F) 表示预测是否正确
对应有 TP TN FP FN

precision(P)为 预测的内容里对了多少 $P = \frac {TP}{TP + FP}$
recall(R)为 gt正确的内容预测出了多少 $R = \frac {TP}{TP + FN}$

## F1
二者都要求高时用 $F_1 = \frac {2 P R} {P + R} $ (两倍并联积在河上飞)

## P-R曲线
按照预测confidence从大到小排序
假设模型直把confidence > T的认为对, 求个 P, R
对于不同的T都算一下
T降必有R不减, 所以可以画出 P-R曲线

## AP
就是 $\int_0^1 P(R) dr$ 即曲线下面积
为了简化计算
设过程中总共有 n 种不同的R
积分改为 $\sum_{k=1}^n (R_k - R_{k-1}) * P_{R_k, max}$
其中 $P_{R_k, max}$ 表示对应 $R=R_k$ 的最大的P的取值

## mAP
多个类别的AP取平均

## Micro vs. Macro

'micro':每个instance具有相同的weight, 此时 TP 就是预测相同, FP=FN 就是预测错误多少个, P和R都用这个算, F1用这样算出的P和R算 (然后发现P=R=F1...)

'macro':每个class具有相同的weight, 此时 TP, FP, TN 按照原定义算, 每类分别算出P和R, $P_{ma}=\overline P, R_{ma}=\overline R, F_{1ma}= \frac {2 P_{ma} R_{ma}} {P_{ma} + R_{ma}}$

# AUC
Area Under roc Curve
ROC是二分类设置不同阈值时, 平面的横坐标是false positive rate(FPR)，纵坐标是true positive rate(TPR)