## Markov Decision process
至于当前状态有关，与历史状态无关
（即使与历史状态有关，也可以把历史状态装进当前状态）
States: $s$
Model: $T(s,a,s')=Pr(s'|s,a)$
Actions: $A(s)$
Reward: $R(s), R(s,a), R(s,a,s')$
Policy: $\pi(s)\to a$
Utility: $U(s)$

## Reward
就像你自己再摸索一个游戏，输了
很难说知道自己在哪里做的不好
可能是最开始失误了，后面走的不错也无法挽回
可能开始很好，最后失误了
也可能一直都走的不好

$R(s)$ immediate reward
$U(s)$ long term reward (delay reward)

$\pi^{*}(s) = argmax_a [\sum_{s'} T(s,a,s')U(s')]$
$U(s) = R(s) + \gamma \pi^{*}(s)$ (end like $\sum_{t=0}^{\infty} \gamma^t R(s_t)$

$\sum_{a=1} b_2$  $\sum_{b}$

