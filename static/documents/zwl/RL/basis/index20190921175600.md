## main difference
1.no superviser, just reward signal
2.delay feedback, not instantaneous

maybe sacrifice immediate reward to gain long-term reward

## agent & state
agent最初是否知道怎么去表示一个状态
(rat experiment)

## agent & enviroment
agent最初是否知道环境是一个怎么样的model
full observation / partial observation
(chess / poke)

## Markov Decision process(MDP)
只与当前状态有关，与历史状态无关
（即使与历史状态有关，也可以把历史状态装进当前状态）

## major component in Agent
Policy : agent's behaviour function
Value : how good is each state or action
Model : agent's representation of the enviroment

States: $s$
Actions: $A(s)$
Model: $T(s,a,s')=Pr(s'|s,a)$ 
Reward: $R(s), R(s,a), R(s,a,s')$ immediate reward
Policy: $\pi(s)\to a$
Utility: $U(s)$ long-term reward
Value: $V(s)$
Quality: $Q(s,a)$
Continuation: $C(s,a)$

## balance  Exploration & Exploitation
基于现有经验做出的选择不一定是最优的
可能一些目前看上去不优的选择，是因为还没有进去探索过，没有怎么尝试过
(restaurant selection,  advertisement selection

## first kind of bellman equation
$\pi^{*}(s) = \max_a [\sum_{s'} T(s,a,s')U(s')]$
$U(s) = R(s) + \gamma \pi^{*}(s)$ (end like $\sum_{t=0}^{\infty} \gamma^t R(s_t)$ (bellman equation)
### way 1: U iteration
start with arbitrary $U_0$
$U_t$ update from $U_{t-1}$ with argmax action

### way 2: $\pi$ iterator
start with arbitrary $\pi_0$
$\pi_t$ update form $U_{t-1}$ with argmax action
$U_t$ was solve base on bellman equation and the selected $\pi_t$
(without max, then it become linear equation, solve U without any initial U)
(end iteration if $\pi$ dosen't change after one iteration)

## second kind of bellman equation
$V(s) = \max_a (R(s,a) + \gamma\sum_{s'} T(s,a,s') V(s'))$
$Q(s,a) = R(s,a) + \gamma\sum_{s'} (T(s,a,s') \max_{a'} Q(s',a'))$
$C(s,a) = \gamma\sum_{s'} T(s,a,s')\max_{a'} (R(s',a')+C(s',a'))$
同一表达式，只不过不同的分段表达方法而已
据说Q比V在RL中好用

## basis
### agent & enviroment
enviroment--s-->agent--a-->enviroment--r-->agent
agent not familiar with enviroment & can only get information by s
only can know enviroment by trying some action and by getting the reward of the action

### plan & conditional plan
plan = fix
conditional plan = care about enviroment change, have if

## evaluation learner
value of policy
experience time(how much data it needs)
computation complexity

## 3 types of RL
1.model based
```
SAR--model learner-->TR--MDP solve-->Q--argmax-->P
     model learner<--TR
```

2.value-function based
```
SAR--value update-->Q--argmax-->P
     value update<--Q
```

3.policy based
```
SAR--policy update-->P
     policy update<--P
```