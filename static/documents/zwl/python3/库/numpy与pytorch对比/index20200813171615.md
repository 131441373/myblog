
### 创建
#### dtype
|numpy|tensor|
|-|-|
|`a = np.array([1,2], dtype='int32')` | `b = torch.tensor([1, 2], dtype=torch.int32)` |

注意训练中, 一般用到 double(float64) 和 long(int64), 否则训练报err给你

#### 构建模式
构造模式  传参都是 tuple 而非 分开写

`ones, zeros, eye` 两者一致
二者对应的 `xxx_like` 也是这样 (即传入一个array/tensor来作为shape)

随机的话 `np.random.random` 和 `torch.rand`

#### 拷贝构造
`=` 按照python传统, 并不是赋值, 而是取引用, 这三者一致

由于np和tensor都是有shape的, 而list是一个随意的嵌套结构
于是拷贝时:
np的`a.copy()` 和 tensor的`b.clone()` 都整个复制了
而 list的`c.copy()`却只和`c[:]`一样, 只负责表层, 要都复制得`copy.deepcopy(a)` import copy

值得一提的是, 类似 `a[1:4] = b[2:5]` 的写法, 三者均能做到拷贝效果而非引用

#### 范围构造
arrange就和range用法一样
linspace(st, ed, num)是构造 闭区间 [st, ed] 上长度为 num 的等差数列

|numpy|tensor|list|
|-|-|-|
|np.arange(10)|torch.arange(10)|list(range(10)|
|np.linspace(2, 3, 10)|torch.linspace(2, 3, 10)|-|

### 索引
|opr|numpy|tensor|list|
|-|-|-|-|
|获得形状|a.shape[i]|b.shape[i] or b.size(i)|len(c)|
|获得元素总数|a.size|b.nelement()|len(c)|

由于np和tensor中的元素都是内部类型, 且是可变对象, 而list中则可以同时存放不可变对象或可变对象
于是切片时
np和tensor都只能引用获取
list中则对第一层不可变对象new, 可变对象引用获取

**重要的是**
np和tensor连 `x = a[0][0]` 这样都获得的是引用, 需要拷贝的话得 `x = a[0][0].clone()`, 
**这里需要特别小心**

值得一提的是, 类似 `a[1:4] = b[2:5]` 的写法, 三者均能做到拷贝效果而非引用

#### boolmask
利用与x同shape的mask, 获取x中对应mask为True的元素 `x[mask]`


### 操作
|numpy|tensor|
|-|-|
|axis=|dim=|

**重要: 这里的操作都不是inplace操作, 都是需要获取返回值的**

#### 拼接

cat是在已有维度上连接, stack是新建一个维度放在那

|numpy|tensor|
|-|-|
|a.concatenate(tuple, axis=)|b.cat(tuple, dim=)|
|a.stack(tuple, axis=)|b.stack(tuple, dim=)|

#### 形状
reshape都是reshape (tensor的话别人的代码常用view
维度重排np是`x.transpose(0, 2, 1)` 而 tensor是`x.permute(0, 2, 1)`　(tensor的transpose只支持两维交换
删维两者都是 `x.squeeze`
扩维 np是`x[np.newaxis, :, :]` , tensor则比较直观直接用 `x.unsqueeze()`

#### 运算
与 python标量 运算 是 逐个运算 这个两者一致, list不行
np与np, tensor与tesnor 进行运算是 对应运算这个两者也一致, list不行

any, sum, cumsum 都一样的
`np.clip` 对应 `torch.clamp`

**关于极值 和 sort, torch都别出心裁**

|opr|numpy|tensor|list|
|-|-|-|-|
|按维度极值|np.max(a, axis=)|torch.max(b, dim=)[0]||
|按维度极值|np.argmax(a, axis=)|torch.max(b, dim=)[1] or torch.argmax(b, dim=)||
|全局最大值|np.max(a)|torch.max(b)||
|排序值|np.sort(a, axis=)不支持倒序, 自己手动切片吧|torch.sort(a, dim=, descending=)[0]|c.sort(reverse=)|
|排序下标|np.argsort(a, axis=)不支持倒序, 自己手动切片吧|torch.sort(a, dim=, descending=)[1] or torch.argsort(a, dim=, descending=)|[x[0] for x in sorted(enumerate(l), key=lambda x:x[1])]|

pytorch 的 **matmul** 也 很高级
1.如果输入两个1维, 则点乘 $u \dot v$
2.如果两个2维, 则矩乘 $AB$
3.第一个1维, 第二个2维, 则 矩乘 $v^T M$ (获得的dim为1, 而不是二维矩阵)
第一个1维, 第二个N>2维, 则 第二个的后两维 矩乘 $v^T M$, 其他维度保存 (类似bmm)
4.第一个2维, 第二个1维, 则 矩乘 $Mv$ (获得的dim为1, 而不是二维矩阵)
第一个N>2维, 第二个1维, 则 第一个的后两维 矩乘 $Mv$, 其他维度保存
5.两个>2维, (最好维数相同, 且除了后两维, 其他维shape一样, 否则不知道想表达什么), 最后两维对应矩乘