excel一样的东西， 一列一个分类，一行一组数据
所以列叫columns, 行叫index

## Series
可理解为字典

用`.values`可以获取值 np.array 
注意不是`values()`

`.index.tolist()` 同理可获得index的 list

用`.tolist()`可以获取值 list

## DataFrame
### 创建
`df=pd.DataFrame(np.array(xxx), index=[..], columns=[..])` 默认index和columns均为从0开始标号的序列
其中np的array二维, 也可以改用二维list

**在默认标号时，行和列的数字均既可作为标号，又可作为label ！！！！！！！！！！！！！！！**

**建议index用num， columns用label**

### 观察
`df.head(num)` 头n个
`df.sample(num)` 随机n个

### 索引
#### label型
`df.loc[label]` 按行获取, `df.loc[:, label]`按列获取
`df.loc[[l1, l2]]` 获取两行, `df.loc[:, [l1, l2]]`获取两列,  `df.loc[[l1, l2], [l3,l4]]` 获取两行两列

#### label型2
只按列选的时候，`df.loc[:, label]` 等于 `df[label]` ， `df.loc[:, [l1, l2]]` 等于 `df[[l1, l2]]`

可用`df[(df['ab']>2) & (df['bc']<5)]` 进行筛选

还可以混用：`df[(df['ab']>2) & (df['bc']<5)]['ab']` 先筛选，然后取出自己有用的列

#### index型
iloc, 用法同label的五种形式

#### 循环遍历
行`for i, data in df.iterrows():`
列`for i, data in df.iteritems():`

这时 data 是 pandas里的Series(视为一维数组), 默认有从0开始的索引
此时既可以用原来column的label去访问, 也可以用第几列这样去访问

#### 条件筛选
`df['label']==xxx`
获得一个表, 满足条件的行为true, 不满足的为false

可以据此筛选出满足条件的行 `df[df['label']==xxx]`
也可以据此筛选出满足条件的行的编号 `df[df['label']==xxx].index`

#### 分组
`groups = df.groupby('Team')`

`for name,group in groups:` 每组一个df

`grouby + apply` 获取Series, 其中传给apply的参数是某组的df

### 修改
#### 设置index从1开始
`df.index=df.index+1`

#### 增
索引后=xxx 表示将所有那个区域的数都改成xxx

`df[label] = np.nan` 可以扩充一列, `df[label] = pd.Series([xx,xx,xx], index=df.index)` 具体的扩充一列
`df.loc[n] = np.nan` 可以扩充一列, `df[label] = pd.Series([xx,xx,xx], index=df.columns)`具体的扩充一行

**小技巧:**
注意这个axis比较难理解
`train['jaccard'] = train[['text', 'selected_text']].apply(lambda x: jaccard_string(x['text'], x['selected_text']), axis=1) # 逐行元素进行运算`
`train['xxx] = train['yyy'].apply(lambda x : x**2) # 单列逐元素运算`
`train['difference'] = train['number of word in T'] - train['number of word in ST']`


#### 删
删行:`df = df.drop(index=xxx)`
删列:`df = df.drop(columns=xxx)`
xxx可以是单个值,也可是list

条件删可以配合 `df[df['label'] == xxx].index`

**如果是Series**, 用`.drop(labels=xxx)`

### 表格形状

#### shape
shape[0] 行
shape[1] 列

#### concat
`pd.concat((a,b),axis=0, ignore_index=True)` 不ignore的话index会重复(不会重标号

默认join='outer', 表示label取并， 表中那个位置没有值的时候补np.nan
如果修改join='inner'， 表示label取交

或写成`a.append(b, ignore_index=True)`, `a.append([b,c], ignore_index=True)`

#### merge
`pd.merge(lhs, rhs, on=['key1', 'key2'])`, how=默认inner

## EDA探索
null
```
df.isnull().sum() # df.isna().sum() 有同样的效果 #因为源于R语言, 所以保留了两种写法

df.dropna(inplace=True)
# 默认 axis=0
# 默认 how='any'表示只要有any就删, 改为all则全null才删
# 默认 inplace=False
```

duplicate
```
df.duplicated().sum() 

df.drop_duplicates(inplace=True)
# 默认 subset=None, 表示整行重复才算重复, 可以设置 ['col1', 'col2'] 表示只要某两行的这些列对应元素重复就算重复
# 默认 keep='first', 对于重复数据保留首个, 可以改为 `last` 或
# 默认inplace=False
```

sample  #观察数据(随机抽
```
df.sample()
```

## 改名:
`data = data.rename(columns={"English words/sentences":"Eng", "French words/sentences":"Frn" })`

## 文件存储
读取`data = pd.read_xxx(path)`
存放`data.to_xxx(path)`

可以用 vim `:set fileencoding` 查看csv的编码
然后用 `pd.read_csv(path, encoding="xxx")`指定编码