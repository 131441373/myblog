直接pip install就好了...(不知道为什么网上的人要这么麻烦才装的了

### basic
pytorch项目中

> scrapy startproject grab

然后它就会生成一个目录

```
grab
  - scrapy.cfg # 配置文件
  - grab
    - spiders
      - __init__.py
    - __init__.py
    - items.py
    - middlewares.py
    - pipelines.py
    - settings.py
```

然后用

> cd grab
> scrapy genspider spiderName url

然后他就会在
`-spiders`中生成一个名为spiderName的py文件

执行时 (此时还在之前cd进的grab文件夹中)

> scrapy crawl spiderName

为了方便, 在pycharm中grab文件夹里新建一个`run.bat`
然后在pycharm edit configuration 里加一个shell
**注意把working directory修改为scrapy的工作目录**

#### settings.py
加入一行 `LOG_LEVEL = "ERROR"` 避免多余输出
`ROBOTSTXT_OBEY` 表示是否遵从网站的robots.txt协议 (一般要调成False, 不然啥也爬不了, 自己注意点就好了)
然后把 `USER_AGENT` 设成 chrome 的 UA 

`"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36"`

将这段代码解注释
```
ITEM_PIPELINES = {
    'grab.pipelines.GrabPipeline': 300, # 300 指优先级
    #可以写多个Pipeline类, 设置不同的优先级
}
```

设置为使用cookie: `COOKIES_ENABLED = True`

`DOWNLOAD_DELAY = 0.3` 避免爬太快被发现

#### items.py
```
# 其中的文件内容(scrapy帮你自动生成的
class GrabItem(scrapy.Item): # Grab是你的Spider文件名
  name1 = scrapy.Field() # 类似于定义variable
  name2 = scrapy.Field()
  # ....
```

#### pipelines.py
```
# 其中的文件内容(scrapy帮你自动生成的
class GrabPipeline:
    def process_item(self, item, spider): # 注意!!!: 该方法每接受到一个item就调用依次
        
        return item
```

### Spider
帮你生成的代码里面的allowed_domains不用就注释掉即可

#### start_requests
默认生成代码给你的start_urls只能是简单的网页
一般要这么写一个函数
```
def start_requests(self):
  yield Request(xxx) # 如加入 cookies=
```

如果仅仅需要cookies, 也可以保留 start_urls, 并列的再定义一个cookies变量
#### parse
response直接可以理解为一个etree, 但是略有不同
`.xpath()` 即可使用xpath
区别在于获得的list里, 每个都是一个Selector类
里面存了 xpath 和 data

response.text 或 response.body.decode(response.encoding) 获取html

|obj|opr1|opr2|
|-|-|-|
|对于Selector的list|get()和extract_first()都是获取第一个元素的data|getall()和extract()都是map成data|
|对于Selector|get()和extract()都是获取data|getall()和extract_first()没意义

可在parse的最后
`yield scrapy.Request(url=, callback=)`
如进行多个页面的爬虫, 爬虫方式相同, 则 `callback=self.parse`
如果先爬摘要, 获取detail的url, 再逐个爬取, 则 `callback=self.parse_detail` (另外写个parse函数)

如有需要, 可以用 `meta={'item':item}` 将当前已经含有部分数据的item传过去, 再callback里parse时, 用 `item = response.meta['item']` 拿回来
有其他类型的信息也可以类似的在meta中以字典形式传过去

#### 持久化存储
如要持久化存储, 在执行的终端命令 `-o filepath`, 但是这个方法很不方便

采用管道操作
在parse的最后
新建一个item对象(具体类名见items.py)
如`item = GrabItem()`
然后用类似dict的访问方式给其中的命名空间赋值
如`item['name1'] = value1`
最后`yield item`

每接受到一个item, 就会按照优先级依次调用各个pipeline的process_item函数
注意item必须要return, 因为第二优先级的pipeline获取item是从第一优先级的pipeline来的

由于每接收一个item就会调用pipeline
所以在pipeline类里还要增加两个函数
`def open_spider(self, spider):`
`def close_spider(self, spider):`

